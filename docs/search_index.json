[["index.html", "GEOG0125: Advanced Topics in Social Geographic Data Science Welcome Description Timetable and key locations Contact details", " GEOG0125: Advanced Topics in Social Geographic Data Science Welcome Welcome to GEOG0125: Advanced Topics in Social Geographic Data Science, one of the core term two modules for this MSc programme (Social and Geographic Data Science). This module has been designed as an advanced topics module to learn data science concepts and methods, and to apply them in the domains of social science and geography. The module will introduce concepts such as Bayesian inference and Machine Learning methodologies. Description This particular facet in the advanced topics course aims to cover an Introduction to Bayesian Inference in RStudio using Stan, which is an interface to RStudio that allows state-of-the-art statistical modelling and Bayesian computation. We will introduce you to the absolute basics of writing your own probabilistic codes for carrying out a broad range of multivariable regression models within the Bayesian framework: Generalised Linear Modelling (GLMs); Hierarchical Models; and most notably for Spatial Risk Models; and Spatiotemporal Risk Models and Bayesian Updating for prediction in risk assessment and uncertainties with exceedance probabilities, which all have significant applications to many fields such as spatial epidemiology, social sciences, or disaster risk reduction and many more. All lecture notes, recommended reading and seminar learning materials as well as supplementary video content will be hosted on this webpage. You can download the lecture notes and data sets for the practical lesson from the table below. Week Downloads Topics 1 [Lecture Notes] Introduction to Bayesian Inference 2 [Lecture Notes] [Datasets] Bayesian Generalised Linear Models (GLMs) 3 [Lecture Notes] [Datasets] Bayesian Hierarchical Models 4 [Lecture Notes] [Datasets] Spatial Risk Models (Part I) 5 [Lecture Notes] [Datasets] Spatiotemproal Risk Models &amp; Bayesian Updating (Part II) Important note: The solutions will be made available by Wednesday emailed via Moodle. Timetable and key locations The Lectures are held every week in-person on Tuesday from 10:00am to 11:00am at the UCL GOSICH’s Wolfson Centre in Room A (Ground Floor) [MAP]. The computer practicals will be at the North West Wing Building in Room G07 [MAP] from 11:00am to 01:00pm on Friday. IMPORTANT NOTE: Please bring your own laptops with you to the computer practicals on Friday. Contact details Dr. Anwar Musah UCL Department of Geography Room 115, North West Wing Building, WC1E 6BT Email: a.musah@ucl.ac.uk "],["reading-list-for-geog0125.html", "Reading List for GEOG0125 Week 1: Introduction to Bayesian Inference Week 2: Bayesian Generalised Linear Models (GLMs) Week 3: Bayesian Hierarchical Models Week 4: Spatial Risk Models (Part I) Week 5: Spatiotemproal Risk Models &amp; Bayesian Updating (Part II) An Excellent &amp; Incredibly Useful Set of Tutorial Videos for Learning Stan Code", " Reading List for GEOG0125 Contact me via email (a.musah@ucl.ac.uk) if you are having problems securing one or any of these recommended books from the UCL library or elsewhere. You can access some of the share reading materials [HERE] Week 1: Introduction to Bayesian Inference Book: [Theory] Slater, M. (2022). Bayesian Methods in Statistics: From Concepts to Practice. Chapters 1: Probability. Pages 1-15. Book: [Theory] Slater, M. (2022). Bayesian Methods in Statistics: From Concepts to Practice. Chapters 2: Probability Distributions (Section: Some common distributions). Pages 24-45. Book: [Theory] Donovan, T.M., &amp; Mickey, R.M. (2019). Bayesian Statistics for Beginners: A Step-by-Step Approach. Chapters 3: Bayes’ Theorem. Pages 29-36. Book: [Theory] Graham, A. (1994). Statistics. Chapters 13: Probability Models. Pages 226-251. Article: [About Stan] Carpenter, B., Gelman, A., et al (2019). Stan: A Probabilistic Programming Language. J Stat Soft. DOI: 10.18637/jss.v076.i01. Book: [Stan programming] Lambert, B. (2018). A Student’s Guide to Bayesian Statistics. Chapters 16: Stan. Week 2: Bayesian Generalised Linear Models (GLMs) Book: [Theory] Slater, M. (2022). Bayesian Methods in Statistics: From Concepts to Practice. Chapters 5: General Models. Pages 114-151. Article: [About regression models and formulation] Baldwin, S.A., &amp; Larson, M.J. (2017). An introduction to using Bayesian linear regression with clinical data. Behaviour Research and Therapy. 98:58-75. DOI: 10.1016/j.brat.2016.12.016. Book: [Theory] Gelman, A et al. (2014). Bayesian Data Analysis (3rd Edition).Chapters 14: Introduction to Regression Models. Pages 353-378. Week 3: Bayesian Hierarchical Models Article: [Tutorial in Stan] Sorensen, T., &amp; Vasishth, S. (2016). Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists. Tutorials in Quantitative Methods for Psychology. 12(3):175-200. DOI: 10.20982/tqmp.12.3.p175 Book: [Theory] Gelman, A et al. (2014). Bayesian Data Analysis (3rd Edition).Chapters 15: Hierarchical Linear Models. Pages 381-402. Week 4: Spatial Risk Models (Part I) Article: [Methodology] Li, L. et al (2022). An ecological study exploring the geospatial associations between socioeconomic deprivation and fire-related dwelling casualties in the England (2010–2019). Applied Geography. 144(1027718). DOI: 10.1016/j.apgeog.2022.102718. Article: [Theory] Morris, M. et al (2019). Bayesian hierarchical spatial models: Implementing the Besag York Mollié model in stan. Spatial and Spatio-temporal Epidemiology. 31(100301). DOI: 10.1016/j.sste.2019.100301 Online Tutorials: [Stan Programming] Morris, M. et al (2019). Spatial Models in Stan: Intrinsic Auto-Regressive Models for Areal Data. URL: https://mc-stan.org/users/documentation/case-studies/icar_stan.html Article: [Methodology] Gomez, M.J. et al (2023). Bayesian spatial modeling of childhood overweight and obesity prevalence in Costa Rica. BMC Public Health. 23(651). DOI:10.1186/s12889-023-15486-1 Article: [History] Besag, J. (1974). Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical Society. Series B (Methodological) (1974): 192-236. Article: [History] Besag, J. &amp; Kooperberg, K. (1995) “On conditional and intrinsic autoregression. Biometrika. 733-746. Article: [History, how the ICAR model was improved] Riebler, A., et al (2016). An intuitive Bayesian spatial model for disease mapping that accounts for scaling. Statistical methods in medical research. 25(4): 1145-1165. DOI: 10.1177/0962280216660421 Week 5: Spatiotemproal Risk Models &amp; Bayesian Updating (Part II) Book: [R-INLA Tutorials] Moraga, P. (2020). Geospatial Health Data: Modeling and Vizualisation with R-INLA and Shiny. Chapters 5: Areal Data. Pages 53-74. Book: [R-INLA Tutorials] Moraga, P. (2020). Geospatial Health Data: Modeling and Vizualisation with R-INLA and Shiny. Chapters 6: Spatial modeling of areal. Lip cancer in Scotland. Pages 75-88. Book: [R-INLA Tutorials] Moraga, P. (2020). Geospatial Health Data: Modeling and Vizualisation with R-INLA and Shiny. Chapters 7: Spatio-temporal modeling of areal data. Lung cancer in Ohio. Pages 93-108. Book: [R-INLA Methodology] Blangiardo, M, &amp; Cameletti, M. (2015). Spatial and Spatio-temporal Bayesian Models in R-INLA. Chapters 6: Spatial modeling. Sections 6.1-6.3, Pages 173-192. Book: [R-INLA Methodology] Blangiardo, M, &amp; Cameletti, M. (2015). Spatial and Spatio-temporal Bayesian Models in R-INLA. Chapters 7: Spatio-temporal models. Sections 7.1-7.2, Pages 253-258. An Excellent &amp; Incredibly Useful Set of Tutorial Videos for Learning Stan Code Bayesian Inference with Stan Episode 1: Motivation [Video] Bayesian Inference with Stan Episode 2: Theory and concepts [Video] Bayesian Inference with Stan Episode 3: Linear Regression [Video] Bayesian Inference with Stan Episode 4: Logistic Regression [Video] "],["getting-started.html", "1 Getting started 1.1 What is Stan? 1.2 Installation of R &amp; RStudio 1.3 Installation of rstan (or Stan) 1.4 Installation of other relevant R-packages needed in GEOG0125", " 1 Getting started 1.1 What is Stan? Stan is an interface for several statistical software packages (e.g., RStudio, Python, Julia, Stata, and MATLAB) which allows the user to perform state-of-the-art statistical modelling within a Bayesian framework. For R users, the package is called rstan which interfaces Stan and RStudio. The focus will be solely on Stan and RStudio. We will show you how one can develop and compile Stan scripts for Bayesian inference through RStudio to perform basic parameter estimation, as well as a wide range of regression-based techniques starting with the simplest univariable linear models and its different families (logistic and Poisson) to the more advanced multivariable spatial risk models. Before all that, let us install the appropriate software. The next section will guide you through the installation process. 1.2 Installation of R &amp; RStudio This section takes you through the installation process for R (Base) and RStudio on MAC and Windows. If you are a MAC user, go to section 1.2.1 If you are a Windows user, jump to section 1.2.2 1.2.1 Installation process for MAC users You will need to have the following software installed for the rstan package to work on MAC. It is recommended to have the latest version of R and RStudio R (version 4.4.2) and RStudio (version 2024.12.0+427) XQuartz (version 2.8.5) XCode (version 16.2) GNU Fortran (version 12.2) If you already installed them, you can jump straight to the section 1.3 to install Stan. [1] Installation of R (4.4.2) and RStudio (2024.12.0-427) on MAC: For R (Base), please ensure you have installed the correct version for your MAC (Intel) or MAC (M1, M2/M3) OS. OS User type R (Base) RStudio Desktop MAC (Intel) R-4.4.2-x86_64.pkg RStudio-2024.12.1-427.dmg MAC (M1, M2 or M3) R-4.4.2-arm64.pkg RStudio-2024.12.1-427.dmg Download the correct version of R (Base) for your system. Double-click on the downloaded file (i.e., R-4.4.2-x86_64.pkg or R-4.4.2-arm64.pkg) and follow the steps to complete the installation. Now, we can download the file (i.e., .dmg) for RStudio from the link provided in the above table. Double-click the downloaded file (i.e., RStudio-2024.12.1-427.dmg) and then drag and drop the RStudio icon into the Applications folder to complete the installation. [2] Installation of XQuartz (2.8.5): Some functions in R (Base) and Stan require some of the libraries from XQuartz in order to function smoothly on your MAC OS. Download the latest version of XQuartz (XQuartz-2.8.5.pkg) by clicking on this LINK and simply complete the installation process by following the steps on your system. [3] Installation of XCode (16.2): Some functions in R (Base) and Stan require some of the external developer tools from the XCode application to function properly on your MAC OS. Go to the App Store application and get the XCode app downloaded by clicking on this LINK. Once it has downloaded, you can click on the “OPEN” button to verify it’s been downloaded. A window will prompt you to complete installation. [4] GNU Fortran (version 12.2): R (Base) and some packages require the GNU Fortran 12.2 compiler to execute smoothly on your MAC OS. Download the latest version of GNU Fortran 12.2 (gfortran-12.2-universal.pkg) by clicking on this LINK and simply complete the installation process by following the steps on your system. This completes the installation process for R and RStudio on MAC. 1.2.2 Installation process for Windows users You will need to have the following software installed for the rstan package to work on Windows. R (version 4.4.2) RTools44 (version 4.4.0) RStudio (version 2023.06.0-421) If you have these installed already - you can jump to the section 1.3 on installing Stan. [1] Installation of R (4.3.2) and RStudio (2023.06.0-421) on Windows: OS User type R (Base) RStudio Desktop Windows R-4.4.2-win.exe RStudio-2024.12.0-427.exe Download the file for R-4.4.2-win.exe attached in the table above. Double-click the downloaded file (i.e., R-4.4.2-win.exe) and follow the steps to complete the installation on your system. Now, we can download the file (i.e., .exe) for RStudio from the link provided in the above table. Double-click the downloaded file (i.e., RStudio-2024.12.0-427.exe) and follow the steps from the installer to complete the installation. [2] Installation of Rtools 4.4.0 For Windows users, after you have completed the installation for R (Base) and RStudio, you are required to install the RTools44 package as it contains some libraries and developer tools for R function properly. Download the latest version of RTools44 by clicking on this LINK to initiate the download of the Rtools42 installer. Double-click the downloaded rtools44-6335-6327.exe file and follow the steps to complete the installation. This completes the installation process for R and RStudio on Windows. 1.3 Installation of rstan (or Stan) When opening the RStudio application on your Windows or MAC PC. You will be greeted with its interface. The window is usual split into three panels: 1.) R Console, 2.) Environments and 3.) Files, Help, Outputs etc., The above section is the Menu Bar. You can access other functions for saving, editing, and opening a new R and Stan script files for writing and compiling codes. Let us opening a new R script by clicking on the File &gt; New File &gt; R Script. This should open a new script file titled “Untitled 1”. Now we are going to latest version of rstan 2.36.0.9000. Using the install.packages() function, we can finally install this package. You can use the code chunk below: install.packages(&quot;rstan&quot;, repos = c(&#39;https://stan-dev.r-universe.dev&#39;, getOption(&quot;repos&quot;))) After installation, use the following code chunk to test if its work: example(stan_model, package = &quot;rstan&quot;, run.dontrun = TRUE) You will first see some gibberish running through your console - don’t be alarmed - it means that its working. You will know rstan has been successfully installed, and working, when you see some iterations for four chains displayed in console. You will also see the objects fit, fit2, mod and stancode stored in the Environments panel when its done. This completes the installation process for rstan. 1.4 Installation of other relevant R-packages needed in GEOG0125 sf: “Simply Features” package that allows the user to load shapefiles into RStudio’s memory. tmap: this package gives access to various functions for users to generate maps. stars: this package for handling spatiotemporal arrays, raster and vector data cubes. SpatialEpi: grants access to function expected() to calculated expected numbers. geostan: grants access to further functions that we need to compute the adjacency matrix that can be handled in Stan. We will use the two functions shape2mat() and prep_icar_data() to create the adjacency matrix as nodes and edges. tidybayes: grants access to further functions for managing posterior estimates. We will need it calculating the exceedance probabilities. Load this alongside tidyverse package. loo: this package allows the user to perform model validation and comparison install.packages(&quot;sf&quot;) install.packages(&quot;tmap&quot;) install.packages(&quot;SpatialEpi&quot;) install.packages(&quot;geostan&quot;) install.packages(&quot;loo&quot;) install.packages(&quot;tidybayes&quot;) install.packages(&quot;tidyverse&quot;) This concludes the installation section and sets you computer up for the course, if you encounter any problems please contact me. "],["introduction-to-bayesian-inference.html", "2 Introduction to Bayesian Inference 2.1 Lecture recording (Length: 1:00:02 minutes) 2.2 Introduction 2.3 Basic building blocks I: About Stan Programming 2.4 Basic building blocks II: Data types and constraint declarations 2.5 Basic building blocks III: Developing our model 2.6 Basic building blocks IV: Compiling our Stan code in RStudio 2.7 Basic building blocks V: Extract posterior samples and interpretation 2.8 Basic building blocks VI: Updating our prediction with new information 2.9 Tasks", " 2 Introduction to Bayesian Inference 2.1 Lecture recording (Length: 1:00:02 minutes) [Watch on YouTube] 2.2 Introduction In this week you will be introduced to Bayesian Statistics. It is a branch of statistics which applies probabilities to statistical problems. The core of Bayesian Statistics is the application of Bayes’ Theorem (or Bayes’ Rule) which uses conditional probabilities to quantify uncertainty outcome. We are going to show you how one can use Stan to encode probabilities to a statistical model (aka likelihood functions) to perform full Bayesian inference. 2.2.1 Learning outcomes Today’s session aims to introduce you to the basic Stan programming etiquette for Bayesian analysis in RStudio using Stan as an Interface, and producing output and interpreting it’s results. By the end of this session, you should be able to perform the following: Develop basic Stan code Know how to write and compile a Stan Program to compute the posterior distribution for simple parameters (i.e., mean, standard deviation, a proportion etc.,) Know how to interpret the results churned from a Stan HMC simulation run You can follow the live walkthrough demonstration delivered in the first 1-hour of the practical, and then use the remaining half to try the practical tutorials and follow the instructions on your own. 2.2.2 Demonstration recording (Length: 38:26 minutes) [Watch on YouTube] 2.2.3 Datasets &amp; setting up the work directory Since, this is our first practical lesson for GEOG0125, let us create a new folder GEOG0125 at the desktop location of our computer. Now, create a sub folder called “Week 1” within the GEOG0125 folder. Here, we will store all our R and Stan scripts. Set your work directory to Week 1’s folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0125/Week 1&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0125/Week 1&quot;) 2.2.4 Loading the appropriate packages Let us load the installed package called rstan. # Load the packages with library() library(&#39;rstan&#39;) Note that when you load rstan from cran you will see some recommendations on using multiple cores for speeding the process. For the best experience, we highly recommend using this code: options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) This tells RStudio to use multiple core for parallel process whenever Stan is being implemented. Every time you want to use Stan - make sure to load parallel::detectCores() and rstan_options code. 2.3 Basic building blocks I: About Stan Programming The section describes how to code up a basic Stan model. This section forms the basis for later, and more complex models. 2.3.1 Opening a Stan Script in RStudio Alright, let’s open a Stan file. You can do this by clicking and selecting File &gt; New File &gt; Stan File When you open a new Stan file, you will be greeted with an untitled script which contains the following bits of code: // // This Stan program defines a simple model, with a // vector of values &#39;y&#39; modeled as normally distributed // with mean &#39;mu&#39; and standard deviation &#39;sigma&#39;. // // Learn more about model development with Stan at: // // http://mc-stan.org/users/interfaces/rstan.html // https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started // // The input data is a vector &#39;y&#39; of length &#39;N&#39;. data { int&lt;lower=0&gt; N; vector[N] y; } // The parameters accepted by the model. Our model // accepts two parameters &#39;mu&#39; and &#39;sigma&#39;. parameters { real mu; real&lt;lower=0&gt; sigma; } // The model to be estimated. We model the output // &#39;y&#39; to be normally distributed with mean &#39;mu&#39; // and standard deviation &#39;sigma&#39;. model { y ~ normal(mu, sigma); } Do not worry about that - it is just formalities by the developers behind rstan. You can delete everything you see in this script as we will build our own basic script from scratch. Once you have deleted the default information save the empty file in the Week 1 folder naming it Predicting a proportion.stan. Whenever you are saving Stan programme in RStudio always make sure to save it with a .stan ending. 2.3.2 Basic structure of a Stan script in RStudio A typical Stan program consist of the following 6 blocks: Data Transformed data Parameters Transformed parameters Model Generated quantities Out of these 6 blocks, the Data, Parameters and Model block must be specified. These are three compulsory blocks needed in any Stan script in order for a Bayesian model to work within the rstan environment. Let us define what these three important blocks are. FIRST: The data block allows the user to declare how the model reads the dataset from RStudio by specifying the sample size N or observations; the number of k parameters that needs to be estimated; the names or list of independent variables for the corresponding parameters (e.g., coefficients); as well as data constraints etc., A data block is specified accordingly as: data { } It is within these curly brackets will specify these details of our dataset. They must be precise as it will have to correspond with the data that is loaded in RStudio’s memory. SECOND: The parameters block allows the use to declare all primitive unknown quantities, including their respective storage types, dimensions, and constraints. The parameters that go here are the ones we want to infer or predict - e.g., includes the mean, variance, sd, coefficient and many more. A parameters block is specified after the data block: data { } parameters { } THIRD: The model block allows the use to declare and specify the sampling statements for the dependent variable (i.e., likelihood function) and parameters (i.e., priors) to be used in the model. A model block is specified after the parameters block: data { } parameters { } model { } Note that adding a double forward slashes // lets the user add a comment to script. Let add comments to the blocks: // Add comments after double forward slashes data { // data block } parameters { // parameters block } model { // model block } Important Notes: Since, the other blocks are not compulsory - we will leave them out for now. But we will come back and explain what those remaining blocks are in Week 2 and 3. Now, save your Stan script. 2.4 Basic building blocks II: Data types and constraint declarations In Stan, all parameters and data must be defined as variables with a specific type. Note, this quite a pain but going through this step allows rstan to perform really fast. There are four basic data types: int: for integers, used for specifying the sample size, and is applied to discrete variables real: for continuous, is applied to continuous variables (i.e., ratio or interval) vector: for a column vector of reals matrix: for a matrix of reals For constraints, we specify them on variables. For example, if we are dealing with a proportion p we will code it as real&lt;lower=0, upper=1&gt; p tells Stan that p can be any value from 0 to 1, inclusive. Note that specifying constraints really help speed Stan up so use them wherever possible. Lastly, you can create arrays of variables. For example, real p[10] tells Stan that p is an array of 10 real values. We can also create a matrix to represent a set of independent variables. Now that we have discussed these points - let work with an actual demonstration to show data types and constraints work. 2.5 Basic building blocks III: Developing our model PROBLEM: In 2021/2022, the GEOG0125 course, there was 13 students enrolled. It was Anwar’s first time being a teacher on the course, and hence was quite curious to know what proportion of students who will pass with a distinction (70%+). Since, he had no data on the pass rates. He had no prior knowledge. First, he assumes that the proportion of those passing with a distinction came from a Beta distribution that mimics a uniform Beta(1.0, 1.0). In term 3, after tedious marking of reports, he observes that 4 students (out of the total: 13) passed with flying colours. What is the posterior distribution of those getting a distinction for GEOG0125? Let us build our first model that predicts the posterior distribution of those passing GEOG0125 with a distinction. In this simple for modelling proportions (or prevalence), we extract our information: Total sample size is 13 (N) Number of students who passed (70% and above) is 4 (p) Data: Proportion (or prevalence) of distinction grades is 4/13, but this is just one instance (it could have been these other likely outcomes: 0/13, 1/13, 2/13, 3/13, 4/13, …, 12/13 or even 13/13). Let us represent proportion of distinction grades with some probability distribution \\(\\theta\\) which has a Binomial distribution (likelihood function). There is no prior knowledge, but we are dealing with a proportion here so a Beta distribution is best for this problem. We are assuming that it has uniform pattern i.e., Beta(1.0, 1.0) because all these instance (i.e., 0/13, 1/13, 2/13, 3/13, 4/13, …, 12/13 or even 13/13) have a equal chance of happening. This is an example of an uninformative prior We can code this information in Stan. FIRST: We specify the total number of student as integer int N which cannot be a negative number in the data block. Also, we also need to specify the number of students that passed as an integer int p which cannot be a negative number in the data block too. data { int&lt;lower = 0&gt; N; int&lt;lower = 0&gt; p; } SECOND: For the parameters block, here we will need to specify the name of the parameter that we want to infer. Here, its \\(\\theta\\) which is the proportion of those who got a distinction in GEOG0125. Note that \\(\\theta\\) follows a beta distribution and is therefore translated as a probability as a real value that is within the range of 0 to 1, inclusive. data { int&lt;lower = 0&gt; N; int&lt;lower = 0&gt; p; } parameters { real&lt;lower=0, upper=1&gt; theta; } THIRD: For the model block, here we will need to define our posterior distributions. Here, we need to state that p likelihood function is sampled from a binomial distribution as a function of N and \\(\\theta\\) (binomial(N, theta)). We also have to say \\(\\theta\\) is sampled from a beta distribution that is uniform. The model block will be: data { int&lt;lower = 0&gt; N; int&lt;lower = 0&gt; p; } parameters { real&lt;lower=0, upper=1&gt; theta; } model { p ~ binomial(N, theta); // our likelihood function or observation model theta ~ beta(1.0, 1.0); // our prior distribution alpha = 1 and beta = 1 } COMPLIMENTS: Well done, we have built our first Bayesian model. Let save it the script, what we need to do is compile and run it through RStudio to get our results. 2.6 Basic building blocks IV: Compiling our Stan code in RStudio Now, let us turn our attention to RStudio. The Stan script needs to be compiled from an R script. We will first need to create a list object using list() to connect the data to the information specified in the data before running the function stan() to the Bayesian model to get the results. The N and p will need to defined in the list object. # create list object and call it dataset dataset &lt;- list(N = 13, p = 4) Now, using the stan() to compile and obtain the posterior estimation of the proportions of students passing with a distinction: # the directory needs to be set to where you save the datasets and Stan script prediction.passes = stan(&quot;Predicting a proportion.stan&quot;, data=dataset, iter=3000, chains=3, verbose = FALSE) Some notes on the above code’s arguments: data= we are pushing the data we created from the list() to the Stan script. Stan is calling it. iter= we are asking the stan() to perform 3,000 iterations on each chain to generate the posterior samples. The chain can be a MCMC, NUTS or HMC algorithm (NUTS No-U-turn sampler is the default) chains= we are asking the stan() function to perform 3 chains (i.e., any of these algorithms can be stated in the function: i.e., MCMC, NUTS &amp; HMC) The resulting output can be printed with the function print(). Here, we are going to print the mean, standard error in mean, SD and the IQR ranges with 95% limits (i.e., 2.5% and 97.5%): print(prediction.passes, probs=c(0.025, 0.25, 0.5, 0.75, 0.975)) We obtain this summary table: Inference for Stan model: Predicting a proportion. 3 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=4500. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 0.33 0.00 0.12 0.12 0.24 0.32 0.41 0.58 1495 1 lp__ -10.08 0.02 0.73 -12.21 -10.26 -9.80 -9.61 -9.55 1879 1 Samples were drawn using NUTS(diag_e) at Thu Jan 12 14:39:10 2023. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). What does it all mean? The top part states that 3 chains were run for 3000 iterations. However, the first 1500 samples generated from each chain were discarded as warm-up, meaning that only 1500 samples from each chain were kept, resulting 4500 (1500x3) total post-warm-up sample draws. The output shows the summary statistics for our \\(\\theta\\). The lp__ is the log-probability - which is used to quantify how well the model is for the data but, in my opinion, its not a useful estimate. Instead, use the effective sample size n_eff and Rhat. If the Rhat is less than 1.05 for All parameters - it means that the estimation of our parameters are fine. 2.7 Basic building blocks V: Extract posterior samples and interpretation At this point, let us extract the posterior samples and graph them to understand it posterior distribution. We use the extract() function from the rstan package, and graph them: # extracting the samples (it should be 4500) theta.pass_draws &lt;- extract(prediction.passes, &#39;theta&#39;)[[1]] # create a graph i.e., histogram hist(theta.pass_draws, xlim = c(0,0.8), ylim = c(0,800), main= &quot;Posterior samples&quot;, ylab = &quot;Posterior Density (Plausibility)&quot;, xlab = expression(paste(&quot;Pass rate &gt; 70%: &quot;, theta, &quot; [%]&quot;))) You can also plot the density as opposed to generating a histogram: plot(density(theta.pass_draws), main = &quot;&quot;, xlab = expression(paste(&quot;Pass rate &gt; 70%: &quot;, theta, &quot; [%]&quot;)), ylab = &quot;Posterior Density (Plausibility)&quot;) We want to compute the posterior mean, with the 0.025 and 0.975 quantiles (these limits are referred to as 95% credibility limits (95% CrI)). Here is how we compute them: # Calculating posterior mean (estimator) mean(theta.pass_draws) # Calculating posterior intervals quantile(theta.pass_draws, probs=c(0.025, 0.975)) Interpretation: The predicted proportion of students passing the GEOG0125 with 70%+ from our sample posterior distribution was 0.3305 (33.05%). This means that 33% is the most plausible pass rate. Our predictions, with 95% credibility, can be within the limits of 0.1239 and 0.5801. Formally writing as \\(\\theta\\) = 33.05% (95% CrI: 12.39-58.01%). We have predicted the most plausible value for pass rate. We can translate this into probability terms. For instance, by asking: “What is the probability that 33% of the students on GEOG0125 will pass with a distinction (70%+)?”. We can use this posterior distribution to make this estimation simply by calculating the mean our samples that is above 0.33. This is known as Exceedance Probability: # what is the probability observing a pass rate for distinction 70%+? mean(theta.pass_draws &gt;= 0.333) [1] 0.4824444 # # what is the probability observing a 33% pass rate for distinction 70%+? mean(theta.pass_draws &gt;= 0.50) [1] 0.09333333 Interpretation: The probability for observing a pass rate of 33% for distinction on GEOG0125 was 48%. It was less likely for half the number of students getting a distinction because the chance were 9.3%. 2.8 Basic building blocks VI: Updating our prediction with new information What about if I receive a new cohort of students in 2022/23. The excellent thing about Bayesian statistics is its ability to update results as new information or data comes - this technique is known as Bayesian Updating. This technique let’s you use the same script and self calibrates the new prior from the previous result. For instance, we predicted 0.3305 as the most plausible value for people to get a distinction. So the beta() specification will be Beta(5, 10) centred on 0.33 and not uniform. We can use past result and data, with the new information to fine tune the prediction, and watch as the results evolve. Let’s have a quick go at updating this prediction with new data. Let’s see: Total sample size is now 26 students in 2022/23 (N) This time around 9 scored a distinction (p) Let us represent the proportion of distinction grades with some probability distribution \\(\\theta\\) with a Binomial distribution (likelihood function) [Binomial(26, 9)]. - We have prior knowledge of last year’s proportion (0.3305) which has a Beta distribution [Beta(5, 10)] - this is our new informative prior - Let updates the codes: Updated dataset in our RScript dataset.updated &lt;- list(N=26, p=9) All we need to do is run the stan() function in R with out new dataset and the old model i.e., prediction.passes specified in the arguments. Obtaining updated posterior samples by compiling new Stan script prediction.passes.updated = stan(fit=prediction.passes, data=dataset.updated, iter=3000, chains=3, verbose = FALSE) print(prediction.passes.updated, probs=c(0.025, 0.25, 0.5, 0.75, 0.975)) Updated output summary table Inference for Stan model: anon_model. 3 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=4500. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 0.36 0.00 0.09 0.20 0.30 0.36 0.42 0.54 1734 1 lp__ -18.74 0.02 0.69 -20.61 -18.91 -18.48 -18.30 -18.25 2017 1 Samples were drawn using NUTS(diag_e) at Fri Jan 17 10:21:26 2025. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Histogram of posterior samples’ distribution # extracting the samples (it should be 4500) theta.pass_draws.new &lt;- extract(prediction.passes.updated, &#39;theta&#39;)[[1]] # create a graph i.e., histogram hist(theta.pass_draws.new, xlim = c(0,1), main= &quot;Posterior samples [Updated]&quot;, ylab = &quot;Posterior Density (Plausibility)&quot;, xlab = expression(paste(&quot;Pass rate (Distinction) Parameter: &quot;, theta, &quot; [%]&quot;))) # create a graph i.e., density plot(density(theta.pass_draws.new), main = &quot;&quot;, xlab = expression(paste(&quot;Pass rate &gt; 70%: &quot;, theta, &quot; [%]&quot;)) , ylab = &quot;Posterior Density (Plausibility)&quot;) Obtain updated posterior proportion and 95% CrI # Calculating posterior mean (estimator) mean(theta.pass_draws.new) [1] 0.3587437 # Calculating posterior intervals quantile(theta.pass_draws.new, probs=c(0.025, 0.975)) 2.5% 97.5% 0.2027673 0.4935101 Interpretation: For the current cohort, the predicted proportion of students passing the GEOG0125 with a distinction from our updated posterior distribution was 0.3587 (36%). Our predictions, with 95% credibility, can be within the limits of 0.2027 and 0.4935. We can formally write this as \\(\\theta\\) = 35.87% (95% CrI: 20.27-49.35%). 2.9 Tasks 2.9.1 Task 1 - Low-level arsenic poisoning in Cornwall, UK Try this first problem in Stan: Suppose, in a small survey, a random sample of 50 villagers from a large population in Cornwall were at risk of arsenic poisoning due to long-term low-level environmental exposure status were selected. Each person’s disease status (i.e., metallic toxicity) was recorded as either Diseased or None. 19 of the respondents have found to be diseased. The distribution of the prevalence is assumed to be of a Beta distribution with Beta(3,5). What is the predicted posterior prevalence of arsenic poisoning in Cornwall and its 95% Credibility intervals? 2.9.2 Task 2 - Body mass index (BMI) problem Try this second problem in Stan: The mean BMI value is 23 with SD of 1.2. Simulate sample of 1000 with BMI values based on this distribution N(23, 1.2) and perform Bayesian inference. What is the posterior mean BMI and its 95% Credibility intervals? Hints: In the R script, use the function rnorm() to generate your sample of 1000 BMI points In the R script, create a list() with N and bmi defined In the Stan script, define the data block in accordance to the list() object In the Stan script, the bmi values are measured outcome. Code this in the model block as a likelihood function using the norm(mu, sigma) notation In the Stan script, use the parameter block, and make sure to code your mu (mean) and sigma (standard deviation) as real (non-negative) numbers Note: Solutions for task 1 and 2 will be made available later today "],["bayesian-generalised-linear-models-glms.html", "3 Bayesian Generalised Linear Models (GLMs) 3.1 Lecture recording (Length: 1:00:02 minutes) 3.2 Introduction 3.3 Poisson Regression Modelling 3.4 Tasks", " 3 Bayesian Generalised Linear Models (GLMs) 3.1 Lecture recording (Length: 1:00:02 minutes) [Watch on YouTube] 3.2 Introduction This week we will learn how to perform Generalised Linear Regression Model (GLMs) within a Bayesian framework. We will use quantitative data focused on crime victimisation in Nigeria to examine how street accessibility measures such as distance, connectivity, choice and integration are associated with reported counts residential burglaries at a street level. The focus will be on Poisson-based regression models with both continuous and categorical independent variables. Let us begin. 3.2.1 Demonstration recording (Length: TBC) 3.2.2 Datasets &amp; setting up the work directory Go to your folder GEOG0125 and create a sub folder called “Week 2” within the GEOG0125 folder. Here, we will store all our R &amp; Stan scripts as well as datasets. Set your work directory to Week 2’s folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0125/Week 2&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0125/Week 2&quot;) The dataset for this practical: Street Burglary Data in Nigeria.csv The datasets for the task at the end of the practical: Obesity and Fastfoods in MSOAs data.csv London LSOA 2015 data.csv 3.2.3 Loading and installing packages We will need to load the following packages for this practical: rstan: a package that enables R to interface with Stan. It provides R the functions to code, parse, compile, test, estimate, and analyze Bayesian models through Stan in R. loo: this package allows the user to perform model validation and comparison. MASS: Gives us access to glm.nb() to estimate the dispersion parameter for use in Bayesian model # Load the packages with library() library(&#39;rstan&#39;) library(&quot;loo&quot;) library(&quot;MASS&quot;) Note that when you load rstan from cran you will see some recommendations on using multiple cores for speeding the process. For the best experience, we highly recommend using this code: options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) This tells RStudio to use multiple core for parallel process whenever Stan is being implemented. Every time you want to use Stan - make sure to load parallel::detectCores() and rstan_options code. 3.3 Poisson Regression Modelling We are going to fit a Poisson-type model on some an outcome that contain counts of households reporting to have been victims of burglary. Let us load the data into RStudio can call the object burglaryDataset. # Set your own directory using setwd() function # Load data into RStudio using read.csv(). The spreadsheet is stored in the object called &#39;burglaryData&#39; burglaryDataset &lt;- read.csv(&quot;Street Burglary Data in Nigeria.csv&quot;) names(burglaryDataset) 3.3.1 Selecting the appropriate Poisson model There are three different types of Poisson models: Standard Poisson regression Negative Binomial Poisson regression Zero-Inflated Poisson regression The implementation of one of these models are highly dependent on how the frequency distribution of the count response variable are displayed. If it resembles a normal curve - then use the standard version. Otherwise, use the Negative Binomial Poisson regression if there is any evidence of over-dispersion. When there is a an inflation of zero counts in the dataset, you will have to use the Zero-Inflated Poisson model to account for that problem. Let’s check the frequency distribution of the outcome variable burglary which corresponds to the number of reported instances a property on a street was burgled. You can simply use a histogram to examine its distribution: # see lowest count min(burglaryDataset$burglary) # see highest count max(burglaryDataset$burglary) # visual distribution hist(burglaryDataset$burglary, breaks=20, xlim = c(0, 25), ylim = c(0, 600), xlab = &quot;Report number of burglaries on a street segment&quot;, ylab = &quot;Frequency&quot;, main = &quot;Distribution of Burglary counts&quot;) The plot show evidence of over-dispersion. It indicates that streets in Kaduna have less frequency of burglaries. Here, we consider using a Negative Binomial Poisson regression model over the standard and zero-inflated versions (i.e., scenario 1 and 3). Now, that we know the model type, let us estimate the over-dispersion parameter using the glm.nb() function. # Fit negative binomial regression null model nb_model &lt;- glm.nb(burglary ~ 1, data = burglaryDataset) # Extract theta theta &lt;- nb_model$theta theta [1] 0.3161472 The estimated over-dispersion parameter is 0.3161472, which is small, this suggests that residential burglaries exhibits substantial over-dispersion. We will use this value in our Bayesian model when we code it in Stan. 3.3.2 Data preparation and set-up for Bayesian analysis in Stan Let begin with a model that only contains independent variables that are continuous measures i.e., distance and connectivity. We can prepare the dataset into list() object: stan_dataset_model1 &lt;- list(N = nrow(burglaryDataset), burg = burglaryDataset$burglary, dist = burglaryDataset$distance, conn = burglaryDataset$connectivity, offset = log(burglaryDataset$totalhouses), phi = 0.31614) Important Notes: The list object stan_dataset_model1 from RStudio is passed into the Stan. N = nrow(burglaryDataset) we are extracting the number of observations present in the dataset. Note that this a smart way instead of hard coding the number. Note that here, N is 743, meaning there 743 street segments. burg = burglaryDataset$burglary: Here, we defined the outcome variable (i.e., counts of burglaries) as burg. dist = burglaryDataset$distance: Independent variable for distance. conn = burglaryDataset$connectivity: Independent variable for connectivity. offset = log(burglaryDataset$totalhouses): Here, the offset is calculated from taking the log-transform of the totalhouses, which is the denominators used for expressing the residential burglaries as a crime rate per capita. phi = 0.31614: We specify the over-dispersion parameter as calculated from glm.nb(). Let’s create our Stan script for running a Negative Binomial Poisson regression within a Bayesian framework. 3.3.3 Creating a script to run a Negative Binomial Poisson regression in Stan A typical Stan program for a regression consist of the following 5 blocks: Data Parameters Model Generated quantities The Data, Parameters and Model block must be specified for the regression to work. But there will be additional block that we will need to transform the resultant parameters (i.e., coefficients) into relative risk (RR) inside the Generated quantities block. Let’s start with the data block: FIRST STEP: We specify the total number of observations N as an integer, as well as the information we defined in our list object stan_dataset_model1 to be passed to Stan. This information is specified in the data block: data { int&lt;lower=0&gt; N; // declare the overall number of data points to be passed into model int&lt;lower=0&gt; burg[N]; // define as an array and specify it as an integer for counts vector[N] dist; // continuous variable vector[N] conn; // continuous variable vector[N] offset; // offset variable for the denominators (total households on a stree segment) real&lt;lower=0&gt; phi; // fixed value to account for overdispersion in the crime counts } SECOND STEP: For the parameters block, here we will need to specify the name of the regression intercept alpha, which is baseline risk of residential burglaries, and the two coefficients i.e., beta[1] and beta[2] for our two independent variables dist and conn respectively. data { int&lt;lower=0&gt; N; // declare the overall number of data points to be passed into model int&lt;lower=0&gt; burg[N]; // define as an array and specify it as an integer for counts vector[N] dist; // continuous variable vector[N] conn; // continuous variable vector[N] offset; // offset variable for the denominators (total households on a stree segment) real&lt;lower=0&gt; phi; // fixed value to account for overdispersion in the crime counts } parameters { real alpha; vector[2] beta; } THIRD STEP: We build our likelihood function and specify the priors for each parameter under the model block. For all parameters - the priors have been centred around 0, meaning that broadly, these variables have no effect on residential burglaries, and if any, these effects may range from ±1 (so in risk terms 0.36 to 2.71). The regression model is neg_binomial_2_log(formula, Overdispersion): data { int&lt;lower=0&gt; N; // declare the overall number of data points to be passed into model int&lt;lower=0&gt; burg[N]; // define as an array and specify it as an integer for counts vector[N] dist; // continuous variable vector[N] conn; // continuous variable vector[N] offset; // offset variable for the denominators (total households on a stree segment) real&lt;lower=0&gt; phi; // fixed value to account for overdispersion in the crime counts } parameters { real alpha; vector[2] beta; } model { // prior specification for our parameters alpha ~ normal(0, 1); beta[1] ~ normal(0, 1); beta[2] ~ normal(0, 1); // likelihood function i.e., statistical model for (i in 1:N) { burg[i] ~ neg_binomial_2_log(alpha + beta[1]*dist[i] + beta[2]*conn[i] + offset[i], phi); } } LAST STEP: We instruct Stan to generated quantitites to calculate the relative risk ratio (RRs) by converting the estimated coefficients by using exp(). We ask it to calculate the log likelihood for model validation: data { int&lt;lower=0&gt; N; // declare the overall number of data points to be passed into model int&lt;lower=0&gt; burg[N]; // define as an array and specify it as an integer for counts vector[N] dist; // continuous variable vector[N] conn; // continuous variable vector[N] offset; // offset variable for the denominators (total households on a stree segment) real&lt;lower=0&gt; phi; // fixed value to account for overdispersion in the crime counts } parameters { real alpha; vector[2] beta; } model { // prior specification for our parameters alpha ~ normal(0, 1); beta[1] ~ normal(0, 1); beta[2] ~ normal(0, 1); // likelihood function i.e., statistical model for (i in 1:N) { burg[i] ~ neg_binomial_2_log(alpha + beta[1]*dist[i] + beta[2]*conn[i] + offset[i], phi); } } generated quantities { // report crime risk ratios real baselineCrimeRR; vector[2] CrimeRR; baselineCrimeRR = exp(alpha); CrimeRR = exp(beta); // model validation and comparison vector[N] log_lik; for (i in 1:N) { log_lik[i] = neg_binomial_2_log_lpmf(burg[i] | alpha + beta[1]*dist[i] + beta[2]*conn[i] + offset[i], phi); } } 3.3.4 Compiling our Stan code in RStudio and Results Now, let us turn our attention to RStudio. Using the stan() to compile and obtain the posterior estimation of the overall risk and crime risk ratios (CRR) for the each independent variable: # the directory needs to be set to where you saved the dataset and Stan script crr.negbin.model1 = stan(&quot;Week_2_Cont_Model.stan&quot;, data=stan_dataset_model1, iter=3000, chains=6, verbose = FALSE) We can print the results accordingly: # reports all results print(crr.negbin.model1, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;baselineCrimeRR&quot;, &quot;CrimeRR&quot;), probs = c(0.025, 0.975)) Output summary table Inference for Stan model: anon_model. 6 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=9000. mean se_mean sd 2.5% 97.5% n_eff Rhat alpha -2.16 0 0.13 -2.42 -1.91 3282 1 beta[1] 0.00 0 0.00 0.00 0.00 4237 1 beta[2] 0.06 0 0.02 0.02 0.11 3216 1 baselineCrimeRR 0.12 0 0.02 0.09 0.15 3225 1 CrimeRR[1] 1.00 0 0.00 1.00 1.00 4236 1 CrimeRR[2] 1.07 0 0.02 1.02 1.11 3215 1 Samples were drawn using NUTS(diag_e) at Fri Jan 24 02:30:42 2025. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Here is the messing part - interpretation. Before anything, note that alpha, beta[1] and beta[2] corresponds to the intercept, and coefficients for distance and connectivity, respectively. These are on the log-scale! The risk ratios from alpha, beta[1] and beta[2] which, in turn, corresponds to the intercept, and coefficients for distance and connectivity, respectively, are the baselineCrimeRR, CrimeRR[1] and CrimeRR[2]. For instance, the significant result is for connectivity, which shows for every unit increase in the number to street connections on a segment in the network, the risk of residential burglaries increase by 1.07 (7%), which is significant based on its 95% Credibility Interval (95 CrI: 1.02-1.11). We can compute exceedance probabilities i.e., that such risk concerning connectivity are greater than 1 (meaning there’s an excess risk): # Here, we can extract the simulated sample for CrimeRR[2] conn_draws &lt;- extract(crr.negbin.model1, &#39;CrimeRR[2]&#39;)[[1]] mean(conn_draws &gt; 1.00) [1] 0.9977778 This indicates a strong chance (99.7%) that streets with more connections to other street segments in the network will certainly increase the risk of residential burglaries. Lastly, we will assess the validity of our model using a test called Leave-One-Out Cross-Validation. The estimate from this test Expected Log Predictive Density (ELPD) quantifies how well a model performs and so higher values of ELPD indicates better predictive performance. While, it can be interpreted on its own, its primary value lies in model comparison rather than as an absolute measure. # extracting ELPD leave-one-out results from model with continuous variables only log_lik_with_cont &lt;- extract_log_lik(crr.negbin.model1, merge_chains = FALSE) r_eff_with_cont &lt;- relative_eff(log_lik_with_cont) loo_cont &lt;- loo(log_lik_with_cont, r_eff_with_cont, cores = 6) print(loo_cont) Computed from 9000 by 743 log-likelihood matrix. Estimate SE elpd_loo -1022.1 37.7 p_loo 2.6 0.4 looic 2044.3 75.3 ------ MCSE of elpd_loo is 0.0. MCSE and ESS estimates assume independent draws (r_eff=1). All Pareto k estimates are good (k &lt; 0.7). See help(&#39;pareto-k-diagnostic&#39;) for details. We take note that the ELPD is -1022.1 3.3.5 Model with categorical variables The purpose of this section is to show: How to include categorical variables into the model Perform model comparisons Let’s include the following categorical variables: integq and choiceq into the regression. Each variable has four categories, we want to omit the first category as it represents the lowest exposure group, and use it a reference for the higher categories when we estimate their risk. # See table for categorical variables table(burglaryDataset$choiceq) table(burglaryDataset$integq) # Create dummy variables for choice burglaryDataset$choicecat2 &lt;- ifelse(burglaryDataset$choiceq == 2, 1, 0) burglaryDataset$choicecat3 &lt;- ifelse(burglaryDataset$choiceq == 3, 1, 0) burglaryDataset$choicecat4 &lt;- ifelse(burglaryDataset$choiceq == 4, 1, 0) # Create dummy variables for integration burglaryDataset$integ2 &lt;- ifelse(burglaryDataset$integq == 2, 1, 0) burglaryDataset$integ3 &lt;- ifelse(burglaryDataset$integq == 3, 1, 0) burglaryDataset$integ4 &lt;- ifelse(burglaryDataset$integq == 4, 1, 0) Let expand the model to include independent variables that are both continuous and categorical measures i.e., distance and connectivity, choice and integration, and the new information into a list() object: stan_dataset_model2 &lt;- list(N = nrow(burglaryDataset), burg = burglaryDataset$burglary, dist = burglaryDataset$distance, conn = burglaryDataset$connectivity, chcat2 = burglaryDataset$choicecat2, chcat3 = burglaryDataset$choicecat3, chcat4 = burglaryDataset$choicecat4, intcat2 = burglaryDataset$integ2, intcat3 = burglaryDataset$integ3, intcat4 = burglaryDataset$integ4, offset = log(burglaryDataset$totalhouses), phi = 0.31614) The amended Stan code will look something as follows: data { int&lt;lower=0&gt; N; // declare the overall number of data points to be passed into model int&lt;lower=0&gt; burg[N]; // define as an array and specify it as an integer for counts vector[N] dist; // continuous variable vector[N] conn; // continuous variable int&lt;lower=0, upper=1&gt; chcat2[N]; int&lt;lower=0, upper=1&gt; chcat3[N]; int&lt;lower=0, upper=1&gt; chcat4[N]; int&lt;lower=0, upper=1&gt; intcat2[N]; int&lt;lower=0, upper=1&gt; intcat3[N]; int&lt;lower=0, upper=1&gt; intcat4[N]; vector[N] offset; // offset variable for the denominators (total households on a stree segment) real&lt;lower=0&gt; phi; // fixed value to account for overdispersion in the crime counts } parameters { real alpha; vector[2] beta; vector[3] chcq; vector[3] intq; } model { // prior specification for our parameters alpha ~ normal(0, 1); beta ~ normal(0, 1); chcq ~ normal(0, 1); intq ~ normal(0, 1); // likelihood function i.e., statistical model for (i in 1:N) { burg[i] ~ neg_binomial_2_log(alpha + beta[1]*dist[i] + beta[2]*conn[i] + chcq[1]*chcat2[i] + chcq[2]*chcat3[i] + chcq[3]*chcat4[i] + intq[1]*intcat2[i] + intq[2]*intcat3[i] + intq[3]*intcat4[i] + offset[i], phi); } } generated quantities { // report crime risk ratios real baselineCrimeRR; vector[2] betaRR; vector[3] chcqRR; vector[3] intqRR; baselineCrimeRR = exp(alpha); betaRR = exp(beta); chcqRR = exp(chcq); intqRR = exp(intq); // model validation and comparison vector[N] log_lik; for (i in 1:N) { log_lik[i] = neg_binomial_2_log_lpmf(burg[i] | alpha + beta[1]*dist[i] + beta[2]*conn[i] + chcq[1]*chcat2[i] + chcq[2]*chcat3[i] + chcq[3]*chcat4[i] + intq[1]*intcat2[i] + intq[2]*intcat3[i] + intq[3]*intcat4[i] + offset[i], phi); } } We can use the stan() to compile the updated code to obtain the posterior estimation accordingly: # the directory needs to be set to where you saved the dataset and Stan script crr.negbin.model2 = stan(&quot;Week_2_Cat_Model.stan&quot;, data=stan_dataset_model2, iter=3000, chains=6, verbose = FALSE) We can print the results accordingly: # reports all results print(crr.negbin.model2, pars = c(&quot;baselineCrimeRR&quot;, &quot;betaRR&quot;, &quot;chcqRR&quot;, &quot;intqRR&quot;), probs = c(0.025, 0.975)) Output summary table Inference for Stan model: anon_model. 6 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=9000. mean se_mean sd 2.5% 97.5% n_eff Rhat baselineCrimeRR 0.13 0 0.03 0.09 0.20 4190 1 betaRR[1] 1.00 0 0.00 1.00 1.00 11128 1 betaRR[2] 1.08 0 0.03 1.03 1.14 5524 1 chcqRR[1] 0.95 0 0.21 0.60 1.44 4692 1 chcqRR[2] 0.94 0 0.23 0.57 1.46 4593 1 chcqRR[3] 0.86 0 0.29 0.44 1.55 4266 1 intqRR[1] 0.94 0 0.20 0.61 1.39 5311 1 intqRR[2] 0.84 0 0.18 0.54 1.25 5119 1 intqRR[3] 0.81 0 0.19 0.49 1.24 5302 1 Samples were drawn using NUTS(diag_e) at Fri Jan 24 03:34:39 2025. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). We can compare this model against the previous one to see which one highest ELPD value for better predictive performance. # extracting ELPD leave-one-out results from model with both continuous and categorical variables log_lik_with_cat &lt;- extract_log_lik(crr.negbin.model2, merge_chains = FALSE) r_eff_with_cat &lt;- relative_eff(log_lik_with_cat) loo_cat &lt;- loo(log_lik_with_cat, r_eff_with_cat, cores = 6) print(loo_cat) Computed from 9000 by 743 log-likelihood matrix. Estimate SE elpd_loo -1026.7 37.7 p_loo 7.7 1.0 looic 2053.4 75.4 ------ MCSE of elpd_loo is 0.0. MCSE and ESS estimates assume independent draws (r_eff=1). All Pareto k estimates are good (k &lt; 0.7). See help(&#39;pareto-k-diagnostic&#39;) for details. This first model’s ELPD was -1022.1, and full model’s show -1026.7. Based on this result, it is clear that the first model is better in terms of predictive performance since its ELPD is higher (i.e., -1022.1 vs -1026.7). So its best to stick with the first model, unless there is a strong theoretical reason to use second model (i.e., accounting for confounding). 3.4 Tasks 3.4.1 Task 1 - Obesity and Fastfoods in London Accessibility to junk food restaurants in young adolescents especially after school hours is a growing cause for concern. Especially, now that many young adults have a sedentary lifestyle; hence obesity rates among this population is increasing in the UK. Try this problem in Stan: Use the dataset Obesity and Fastfoods in MSOAs data.csv to determine the links between prevalence of obesity in high school students and density of fast food (cheap) restaurant and deprivation in MSOAs in London. Implement a Bayesian GLM using Stan code. Variable names: SEQID: ID number for row MSOA11CD: Unique identifier for MSOA area MSOA11NM: Name of the MSOA area OBESE: Number of child identified as obese in MSOA in London TOTAL: Total number of children surveyed for BMI measurements IMDMSOA: Area-level socioeconomic deprivation score (higher scores means higher deprivation and vice versa) RESTCAT: Categorical variable for classifying an MSOA in terms of density of junk/cheap fast food outlets restaurants: 1 = 1 to 10, 2= 11 to 25, 3= 26 to 50 and 4= 51 to 300. HINT: You might want to consider using the following functions: binomial_logit() or binomial() in the model block and reporting the odd ratios using the generated quantities block. You might want to consider computing the exceedance probabilities for the odd ratios using the threshold of 1. 3.4.2 Task 2 - Factors affecting house prices in London (2015) Try this problem in Stan: Use London LSOA 2015 data.csv data pertained house prices in 2015, and assess it’s relationship with public transport accessibility (PTA), average income and socioeconomic deprivation (IMD) as the independent variables. Implement a Bayesian GLM using Stan code. Variables names: LSOACODE: Unique identification code for the geographic area AVEPRICE: (Dependent variable) Average house price estimated for the LSOA in 2015 AVEINCOME: Estimated average annual income for households within an LSOA in 2015 IMDSCORE: Deprivation score for an LSOA in 2015 PTAINDEX: Measures levels of access/connectivity to public transport HINT: You might want to consider using the following functions: normal() in the model block. You might want to consider computing the exceedance probabilities for the coefficients using the threshold of 0. Note: Solutions will be made available later today. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
