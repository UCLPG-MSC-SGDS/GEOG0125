[["index.html", "GEOG0125: Advanced Topics in Social Geographic Data Science Welcome Description Timetable and key locations Contact details", " GEOG0125: Advanced Topics in Social Geographic Data Science Welcome Welcome to GEOG0125: Advanced Topics in Social Geographic Data Science, one of the core term two modules for this MSc programme (Social and Geographic Data Science). This module has been designed as an advanced topics module to learn data science concepts and methods, and to apply them in the domains of social science and geography. The module will introduce concepts such as Bayesian inference and Machine Learning methodologies. Description This particular facet in the advanced topics course aims to cover an Introduction to Bayesian statistics in RStudio using Stan, which is an interface to RStudio that allows state-of-the-art statistical modelling and Bayesian computation. We will introduce you to the absolute basics of writing your own probabilistic codes for carrying out a broad range of multivariable models within the Bayesian framework: Generalised Linear Modelling (GLMs); Hierarchical Models; and Generalized Additive Models (GAMs). Thereafter, you will be shown how to create Spatial &amp; Spatiotemporal Bayesian models using Conditional Autoregression (CARs) for risk prediction and uncertainty using exceedance probabilities, which have significant applications to many fields such as spatial epidemiology, social sciences, or disaster risk reduction and many more. All lecture notes, recommended reading and seminar learning materials as well as supplementary video content developed by Dr. Anwar Musah will be hosted on this webpage. You can download the lecture notes and data sets for the practical lesson from the table below. Week Downloads Topics 1 [Lecture Notes] Introduction to Bayesian Statistics 2 [Lecture Notes] [Datasets] Bayesian Generalised Linear Models (GLMs) 3 [Lecture Notes] [Datasets] Bayesian Generalised Additive Models (GAMs) 7 [Lecture Notes] [Datasets] Bayesian Hierarchical Regression Models 8 [Lecture Notes] [Datasets] Bayesian Spatial Risk Models 9 [Lecture Notes] [Datasets] Spatiotemproal Modelling 10 [Lecture Notes] Study Design, Research &amp; Revision Solutions: [Week 1] | [Week 2] | [Week 3] | [Week 7] | [Week 8] | [Week 9] Important note: The solutions will be made available each week after the seminars are completed. Timetable and key locations The Lectures are held every week in-person on Tuesday from 10:00am to 11:00am at the North West Wing Building (Room G07) [MAP]. The computer seminar practicals will be at the same location from 11:00am to 01:00pm on Friday. IMPORTANT NOTE: Please bring your own laptops with you to the computer practicals on Friday. Contact details Dr. Anwar Musah UCL Department of Geography Room 115, North West Wing Building, WC1E 6BT Email: a.musah@ucl.ac.uk "],["reading-list-for-geog0125.html", "Reading List for GEOG0125 Week 1: Introduction to Bayesian Inference Week 2: Bayesian Generalised Linear Models (GLMs) Week 3: Bayesian Generalised Additive Models (GAMs) Week 7: Bayesian Hierarchical Regression Models Week 8: Spatial Intrinsic Conditional Autoregressive Modelling (ICAR) Week 9: Bayesian Updating for Spatiotemporal Areal Analysis Week 10: Study Design, Research Methodology &amp; Revision Excellent &amp; Incredibly Useful Tutorial Videos for Learning Stan Code 4-Day Summer Workshop: Introduction to Bayesian Inference and Modelling [2022/23]", " Reading List for GEOG0125 Contact me via email (a.musah@ucl.ac.uk) if you are having problems securing one or any of these recommended books from the UCL library or elsewhere. You can access some of the share reading materials [HERE] Week 1: Introduction to Bayesian Inference Book: [Theory] Slater, M. (2022). Bayesian Methods in Statistics: From Concepts to Practice. Chapters 1: Probability. Pages 1-15. Book: [Theory] Slater, M. (2022). Bayesian Methods in Statistics: From Concepts to Practice. Chapters 2: Probability Distributions (Section: Some common distributions). Pages 24-45. Book: [Theory] Donovan, T.M., &amp; Mickey, R.M. (2019). Bayesian Statistics for Beginners: A Step-by-Step Approach. Chapters 3: Bayes’ Theorem. Pages 29-36. Book: [Theory] Graham, A. (1994). Statistics. Chapters 13: Probability Models. Pages 226-251. Article: [About Stan] Carpenter, B., Gelman, A., et al (2019). Stan: A Probabilistic Programming Language. J Stat Soft. DOI: 10.18637/jss.v076.i01. Book: [Stan programming] Lambert, B. (2018). A Student’s Guide to Bayesian Statistics. Chapters 16: Stan. Week 2: Bayesian Generalised Linear Models (GLMs) Book: [Theory] Slater, M. (2022). Bayesian Methods in Statistics: From Concepts to Practice. Chapters 5: General Models. Pages 114-151. Article: [About regression models and formulation] Baldwin, S.A., &amp; Larson, M.J. (2017). An introduction to using Bayesian linear regression with clinical data. Behaviour Research and Therapy. 98:58-75. DOI: 10.1016/j.brat.2016.12.016. Book: [Theory] Gelman, A et al. (2014). Bayesian Data Analysis (3rd Edition).Chapters 14: Introduction to Regression Models. Pages 353-378. Week 3: Bayesian Generalised Additive Models (GAMs) Online: [Tutorial]. Fitting GAMs with brms: Part 1 a Simple GAM. LINK: https://fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/ Online: [Tutorial] Chapter 2: Interpreting and Visualizing GAMs. LINK: https://noamross.github.io/gams-in-r-course/chapter2 Book: [Theory] Wood, S.N. (2017). Generalized Additive Models: An Introduction with R (2nd Edition). Chapter 4: Introducing GAMs. Pages 161-191. Book: [Theory and Applications] Wood, S.N. (2017). Generalized Additive Models: An Introduction with R (2nd Edition). Chapter 7: GAMs in Practice. Pages 325-398. Dissertation [Application] Qi, G. (2023). Investigating the Crime Recovery Patterns in Nottingham in the Post-Lockdown Period using Social Disorganisation Theory. UCL Undergraduate Dissertation 2022/23. Department of Geography. Submitted BA Geography with Social Data Sciences. Download: [CLICK] Week 7: Bayesian Hierarchical Regression Models Article: [Tutorial in Stan] Sorensen, T., &amp; Vasishth, S. (2016). Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists. Tutorials in Quantitative Methods for Psychology. 12(3):175-200. DOI: 10.20982/tqmp.12.3.p175 Book: [Theory] Gelman, A et al. (2014). Bayesian Data Analysis (3rd Edition).Chapters 15: Hierarchical Linear Models. Pages 381-402. Week 8: Spatial Intrinsic Conditional Autoregressive Modelling (ICAR) Article: [Methodology] Li, L. et al (2022). An ecological study exploring the geospatial associations between socioeconomic deprivation and fire-related dwelling casualties in the England (2010–2019). Applied Geography. 144(1027718). DOI: 10.1016/j.apgeog.2022.102718. Article: [Theory] Morris, M. et al (2019). Bayesian hierarchical spatial models: Implementing the Besag York Mollié model in stan. Spatial and Spatio-temporal Epidemiology. 31(100301). DOI: 10.1016/j.sste.2019.100301 Online Tutorials: [Stan Programming] Morris, M. et al (2019). Spatial Models in Stan: Intrinsic Auto-Regressive Models for Areal Data. URL: https://mc-stan.org/users/documentation/case-studies/icar_stan.html Article: [Methodology] Gomez, M.J. et al (2023). Bayesian spatial modeling of childhood overweight and obesity prevalence in Costa Rica. BMC Public Health. 23(651). DOI:10.1186/s12889-023-15486-1 Article: [History] Besag, J. (1974). Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical Society. Series B (Methodological) (1974): 192-236. Article: [History] Besag, J. &amp; Kooperberg, K. (1995) “On conditional and intrinsic autoregression. Biometrika. 733-746. Article: [History, how the ICAR model was improved] Riebler, A., et al (2016). An intuitive Bayesian spatial model for disease mapping that accounts for scaling. Statistical methods in medical research. 25(4): 1145-1165. DOI: 10.1177/0962280216660421 Week 9: Bayesian Updating for Spatiotemporal Areal Analysis [TBC] Week 10: Study Design, Research Methodology &amp; Revision [TBC] Excellent &amp; Incredibly Useful Tutorial Videos for Learning Stan Code Bayesian Inference with Stan Episode 1: Motivation [Video] Bayesian Inference with Stan Episode 2: Theory and concepts [Video] Bayesian Inference with Stan Episode 3: Linear Regression [Video] Bayesian Inference with Stan Episode 4: Logistic Regression [Video] 4-Day Summer Workshop: Introduction to Bayesian Inference and Modelling [2022/23] UCL SODA CPD Workshop [SOURCE] Day 1: Introduction to Bayesian Inference using Stan [Lecture Video] [Live Demonstration] Day 2: Bayesian Generalised Linear Models using Stan [Lecture Video] [Live Demonstration] Day 3: Bayesian Generalised Hierarchical Regression Models [Lecture Video (Part 1)][Lecture Video (Part 2)][Live Demonstration] Day 4: Bayesian Spatial Modelling for Areal Data in Stan [Lecture Video] [Live Demonstration] "],["getting-started.html", "1 Getting started 1.1 What is Stan? 1.2 Installation of R &amp; RStudio 1.3 Installation of rstan (or Stan) 1.4 Installation of other relevant R-packages", " 1 Getting started 1.1 What is Stan? Stan is an interface for several statistical software packages (e.g., RStudio, Python, Julia, Stata, and MATLAB) which allows the user to perform state-of-the-art statistical modelling within a Bayesian framework. For R users, the package is called rstan which interfaces Stan and RStudio. The focus of this workshop will be solely on Stan and RStudio. We will show you how one can develop and compile Stan scripts for Bayesian inference through RStudio to perform basic parameter estimation, as well as a wide range of regression-based techniques starting with the simplest univariable linear models and its different families to the more advanced multivariable spatial risk models. Before all that, let us install the appropriate software. The next section will guide you through the installation process. Important note: The installation process is a bit involved for all the wrong reasons. For some strange reason, Stan (i.e., rstan package) works perfectly with any previous release of R that’s version 4.2.x. However, rstan don’t seem to work properly with the latest builds for R that’s version 4.3.x. So to avoid grief, do not install any version of R that’s version 4.3.x. If you have R 4.3.x installed. Please remove it, as well as we recommend the removal of any historical folders that are R-program-based hidden in your machine as safety measure to avoid any conflicts with the things we are about to install. 1.2 Installation of R &amp; RStudio This section takes you through the installation process for R (Base) and RStudio on MAC and Windows. 1.2.1 Installation process for MAC users You will need to have the following software installed for the rstan package to work on MAC. R (version 4.2.3) RStudio (version 2023.06.0-421) XQuartz (version 2.8.5) XCode (version 14.3.1 (14E300c)) If you already have R and RStudio installed, then the first thing to do is to remove them entirely from our MAC machine. Please follow the guidance outlined here. Removal steps: Before deleting R and RStudio. We need to find this folder called “R.framework” on our MAC PC and delete it from our machine entirely, so that when we install R (version 4.2.3) and rstan there would not be any crashes or conflicts between the versions, packages etc., To find the “R.framework” folder on a MAC PC: Go your “Finder” application and click it. Click “Go” in the menu bar. Click “Go to Folder” on the pull down menu and a panel will appear. Type /Library/Frameworks/ in the panel and press the return (or enter) button in the search panel to go the “Frameworks” folder. From the list of folders under “Frameworks”. Highlight “R.framework” folder and right-click then delete it. Go back to your Applications folder and delete R and RStudio apps and empty out your Bin Installation of R (4.2.3) and RStudio (2023.06.0-421) on MAC: OS User type R (Base) RStudio Desktop MAC R-4.2.3.pkg RStudio-2023.06.0-421.dmg Download the file for R-4.2.3.pkg attached in the table above. Double-click the downloaded file (i.e., R-4.2.3.pkg) and follow the steps to complete the installation. Now, we can download the file (i.e., .dmg) for RStudio from the link provided in the above table. Double-click the downloaded file (i.e., RStudio-2023.06.0-421.dmg) and then drag and drop the RStudio icon into the Applications folder to complete the installation. Installation of XQuartz (2.8.5): Some functions in R and Stan require some of the libraries from XQuartz to function properly on your MAC. Download the latest version of XQuartz (XQuartz-2.8.5.pkg) by clicking on this LINK and simply complete the installation process by following the steps. Installation of XCode (14.3.1 [14E300c]): Some functions in R and Stan require some of the external developer tools from XCode application to function properly on your MAC. Go to the App Store application and get the XCode app downloaded by clicking on this LINK. Once it is downloaded, you can click on the “OPEN” button to verify it’s been downloaded. A window will prompt you to complete installation. Lastly, and for safe measures - we going to run this through our Terminal. You can open the Terminal program by going to the Applications &gt; Utilities folder and select the Terminal application In the terminal, type the following code xcode-select --install. If you get the following error message shown in the code chunk below, then it means that the XCode program has been installed properly - no need to do anything at this point. Otherwise, the terminal will proceed to install the XCode tool remotely. xcode-select: error: command line tools are already installed, use &quot;Software Update&quot; in System Settings to install updates This completes the installation process for R and RStudio on MAC. 1.2.2 Installation process for Windows users You will need to have the following software installed for the rstan package to work on Windows. R (version 4.2.3) Rtools42 (version 4.2) RStudio (version 2023.06.0-421) If you already have R and RStudio installed, then the first thing to do is to remove them entirely from our Windows machine. Please follow the guidance outlined here: Removal steps Type “Add or remove programs” in the bottom search bar. The “Add or remove programs” settings should appear. Click it to show the “Apps &amp; Features” window. Under this section – completely uninstall R (4.3.x). Repeat the third steps for removing RStudio and Rtools 4.3. Now, we going to delete any folders with R-related programs which are hidden in our computer. We will check three locations in our PC (i.e., Local, Program Files or Programs Files (x86) folder). Type the path C:\\Users\\Your_username\\AppData\\Local in the search bar on the File Explorer window and press “Enter”. It should take you to the “Local” folder. Here, delete the following folders if you see them: “R”, “RStudio” and/or “RStudio-Desktop”. Repeat the steps in 5 by typing the path C:\\Program Files in this search bar on the File Explorer window. Pressing “Enter” will direct you to the “Program Files” folder. Again, check and delete any folders that’s “R”, “RStudio” and/or “RStudio-Desktop”. Lastly, repeat the steps in 5 or 6 by checking the folder “Program Files (x86)” using this path C:\\Program Files (x86) and delete accordingly. Installation of R (4.3.2) and RStudio (2023.06.0-421) on Windows: OS User type R (Base) RStudio Desktop Windows R-4.2.3-win.exe RStudio-2023.06.0-421.exe Download the file for R-4.2.3-win.exe attached in the table above. Double-click the downloaded file (i.e., R-4.2.3-win.exe) and follow the steps to complete the installation. Now, we can download the file (i.e., .exe) for RStudio from the link provided in the above table. Double-click the downloaded file (i.e., RStudio-2023.06.0-421.exe) and follow the steps from the installer to complete the installation. Installation of Rtools 4.2 For Windows users, after you have completed the installation for R and RStudio, you are required to install the Rtools42 package as it contains some libraries and developer tools for the smooth functioning of Stan. Download the latest version of Rtools42 by clicking on this LINK to initiate the download of the Rtools42 installer. Double-click the downloaded rtools42-5355-5357.exe file and follow the steps to complete the installation. This completes the installation process for R and RStudio on Windows. 1.3 Installation of rstan (or Stan) When opening the RStudio application on your Windows or MAC PC. You will be greeted with its interface. The window is usual split into three panels: 1.) R Console, 2.) Environments and 3.) Files, Help, Outputs etc., The above section is the Menu Bar. You can access other functions for saving, editing, and opening a new R and Stan script files for writing and compiling codes. Let us opening a new R script by clicking on the File &gt; New File &gt; R Script. This should open a new script file titled “Untitled 1”. Now we are going to latest development version of rstan 2.26.22. Using the install.packages() function, we can finally install this package. You can use the code chunk below: install.packages(&quot;rstan&quot;, repos = c(&quot;https://mc-stan.org/r-packages/&quot;, getOption(&quot;repos&quot;))) After installation, use the following to code chunk to test if its work: example(stan_model, package = &quot;rstan&quot;, run.dontrun = TRUE) You will first see some gibberish running through your console - don’t be alarmed - it means that its working. You will know rstan has been successfully installed and it works when you see some iterations for four chains displayed in console. You will also see the objects fit, fit2, mod and stancode stored in the Environments panel when its done. This completes the installation process for rstan. 1.4 Installation of other relevant R-packages You will need to install the following packages ahead of next week’s sessions: sf: “Simply Features” package that allows the user to load shapefiles into RStudio’s memory. tmap: this package gives access to various functions for users to generate maps. devtools: Needed to select a particular version of the stars package. stars: Needed as support for sf and tmap packages to work. SpatialEpi: grants access to function expected() to calculated expected numbers. geostan: grants access to further functions that we need to compute the adjacency matrix that can be handled in Stan. We will use the two functions shape2mat() and prep_icar_data() to create the adjacency matrix as nodes and edges. tidybayes: grants access to further functions for managing posterior estimates. We will need it calculating the exceedance probabilities. Load this alongside tidyverse package. install.packages(&quot;devtools&quot;) devtools::install_version(&quot;stars&quot;, version = &quot;0.5.3&quot;, repos = &quot;http://cran.us.r-project.org&quot;) install.packages(&quot;sf&quot;) install.packages(&quot;tmap&quot;) install.packages(&quot;SpatialEpi&quot;) install.packages(&quot;geostan&quot;) install.packages(&quot;tidybayes&quot;) install.packages(&quot;tidyverse&quot;) This concludes the installation section and sets you computer up for the course. If you encounter any problems please contact me ahead of next week. "],["introduction-to-bayesian-statistics.html", "2 Introduction to Bayesian Statistics 2.1 Lecture recording (Length: 59:50 minutes) 2.2 Introduction 2.3 Basic building blocks I: About Stan Programming 2.4 Basic building blocks II: Data types and constraint declarations 2.5 Basic building blocks III: Developing our model 2.6 Basic building blocks IV: Compiling our Stan code in RStudio 2.7 Basic building blocks V: Extract posterior samples and interpretation 2.8 Basic building blocks VI: Updating our prediction with new information 2.9 Tasks", " 2 Introduction to Bayesian Statistics 2.1 Lecture recording (Length: 59:50 minutes) Important Notes: If this or any subsequent video don’t play from this website, then try playing it directly from Microsoft Streams - you should be able to view it by clicking this [LINK]. Access to Microsoft Steams to these videos will require use of your UCL institutional login details. 2.2 Introduction In this week you will be introduced to Bayesian Statistics. It is a branch of statistics which applies probabilities to statistical problems. The core of Bayesian Statistics is the application of Bayes’ Theorem (or Bayes’ Rule) which uses conditional probabilities to quantify uncertainty outcome. We are going to show you how one can use Stan to encode probabilities to a statistical model (aka likelihood functions) to perform full Bayesian inference. 2.2.1 Learning outcomes Today’s session aims to introduce you to the basic Stan programming etiquette for Bayesian analysis in RStudio using Stan as an Interface, and producing output and interpreting it’s results. By the end of this session, you should be able to perform the following: Develop basic Stan code Know how to write and compile a Stan Program to compute the posterior distribution for simple parameters (i.e., mean, standard deviation, a proportion etc.,) Know how to interpret the results churned from a Stan HMC simulation run You can follow the live walkthrough demonstration delivered in the first 1-hour of the practical, and then use the remaining half to try the practical tutorials and follow the instructions on your own. 2.2.2 Demonstration recording (Length: 38:23 minutes) Important Notes: If this or any subsequent video don’t play from this website, then try playing it directly from Microsoft Streams - you should be able to view it by clicking this [LINK]. Access to Microsoft Steams to these videos will require use of your UCL institutional login details. 2.2.3 Datasets &amp; setting up the work directory Since, this is our first practical lesson for GEOG0125, let us create a new folder GEOG0125 at the desktop location of our computer. Now, create a sub folder called “Week 1” within the GEOG0125 folder. Here, we will store all our R and Stan scripts. Set your work directory to Week 1’s folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0125/Week 1&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0125/Week 1&quot;) 2.2.4 Loading and installing packages To start writing scripts for Bayesian analysis, we will need to install a package called rstan. # setting timer over 100s to extend timeout::: just in case options(timeout=100) # install package rstan with its dependency install.packages(&quot;rstan&quot;, repos=&quot;https://cloud.r-project.org&quot;, dependencies = TRUE) # Load the packages with library() library(&#39;rstan&#39;) Note that when you load rstan from cran you will see some recommendations on using multiple cores for speeding the process. For the best experience, we highly recommend using this code: options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) This tells RStudio to use multiple core for parallel process whenever Stan is being implemented. Every time you want to use Stan - make sure to load parallel::detectCores() and rstan_options code. 2.3 Basic building blocks I: About Stan Programming The section describes how to code up a basic Stan model. This section forms the basis for later, and more complex models. 2.3.1 Opening a Stan Script in RStudio Alright, let’s open a Stan file. You can do this by clicking and selecting File &gt; New File &gt; Stan File When you open a new Stan file, you will be greeted with an untitled script which contains the following bits of code: // // This Stan program defines a simple model, with a // vector of values &#39;y&#39; modeled as normally distributed // with mean &#39;mu&#39; and standard deviation &#39;sigma&#39;. // // Learn more about model development with Stan at: // // http://mc-stan.org/users/interfaces/rstan.html // https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started // // The input data is a vector &#39;y&#39; of length &#39;N&#39;. data { int&lt;lower=0&gt; N; vector[N] y; } // The parameters accepted by the model. Our model // accepts two parameters &#39;mu&#39; and &#39;sigma&#39;. parameters { real mu; real&lt;lower=0&gt; sigma; } // The model to be estimated. We model the output // &#39;y&#39; to be normally distributed with mean &#39;mu&#39; // and standard deviation &#39;sigma&#39;. model { y ~ normal(mu, sigma); } Do not worry about that - it is just formalities by the developers behind rstan. You can delete everything you see in this script as we will build our own basic script from scratch. Once you have deleted the default information save the empty file in the Week 1 folder naming it Predicting a proportion.stan. Whenever you are saving Stan programme in RStudio always make sure to save it with a .stan ending. 2.3.2 Basic structure of a Stan script in RStudio A typical Stan program consist of the following 6 blocks: Data Transformed data Parameters Transformed parameters Model Generated quantities Out of these 6 blocks, the Data, Parameters and Model block must be specified. These are three compulsory blocks needed in any Stan script in order for a Bayesian model to work within the rstan environment. Let us define what these three important blocks are. FIRST: The data block allows the user to declare how the model reads the dataset from RStudio by specifying the sample size N or observations; the number of k parameters that needs to be estimated; the names or list of independent variables for the corresponding parameters (e.g., coefficients); as well as data constraints etc., A data block is specified accordingly as: data { } It is within these curly brackets will specify these details of our dataset. They must be precise as it will have to correspond with the data that is loaded in RStudio’s memory. SECOND: The parameters block allows the use to declare all primitive unknown quantities, including their respective storage types, dimensions, and constraints. The parameters that go here are the ones we want to infer or predict - e.g., includes the mean, variance, sd, coefficient and many more. A parameters block is specified after the data block: data { } parameters { } THIRD: The model block allows the use to declare and specify the sampling statements for the dependent variable (i.e., likelihood function) and parameters (i.e., priors) to be used in the model. A model block is specified after the parameters block: data { } parameters { } model { } Note that adding a double forward slashes // lets the user add a comment to script. Let add comments to the blocks: // Add comments after double forward slashes data { // data block } parameters { // parameters block } model { // model block } Important Notes: Since, the other blocks are not compulsory - we will leave them out for now. But we will come back and explain what those remaining blocks are in Week 2 and 3. Now, save your Stan script. 2.4 Basic building blocks II: Data types and constraint declarations In Stan, all parameters and data must be defined as variables with a specific type. Note, this quite a pain but going through this step allows rstan to perform really fast. There are four basic data types: int: for integers, used for specifying the sample size, and is applied to discrete variables real: for continuous, is applied to continuous variables (i.e., ratio or interval) vector: for a column vector of reals matrix: for a matrix of reals For constraints, we specify them on variables. For example, if we are dealing with a proportion p we will code it as real&lt;lower=0, upper=1&gt; p tells Stan that p can be any value from 0 to 1, inclusive. Note that specifying constraints really help speed Stan up so use them wherever possible. Lastly, you can create arrays of variables. For example, real p[10] tells Stan that p is an array of 10 real values. We can also create a matrix to represent a set of independent variables. Now that we have discussed these points - let work with an actual demonstration to show data types and constraints work. 2.5 Basic building blocks III: Developing our model PROBLEM: Last year, in the advanced GEOG0125 course, there was 13 students enrolled. It was Anwar’s first time being a teacher on the course, and hence was quite curious to know what proportion of students will pass with a distinction. Since, he has no data of the pass rates - therefore has no prior knowledge. First, he assumes that the proportion of those passing with a distinction comes from a Beta distribution that is uniform Beta(1.0, 1.0). In term 3, after tedious marking of reports, he observes that 4 students (out of the total: 13) passed with flying colours. What is the posterior distribution of those getting a distinction for GEOG0125? Let us build our first model that predicts the posterior distribution of those passing GEOG0125 with a distinction. In this simple for modelling proportions (or prevalence), we extract our information: Total sample size is 13 (N) Number of students who passed (70% and above) is 4 (p) Data: Proportion (or prevalence) of distinction grades is 4/13, but this is just one instance (it could have been these other likely outcomes: 0/13, 1/13, 2/13, 3/13, 4/13, …, 12/13 or even 13/13). Let us represent proportion of distinction grades with some probability distribution \\(\\theta\\) which has a Binomial distribution (likelihood function). There is no prior knowledge, but we are dealing with a proportion here so a Beta distribution is best for this problem. We are assuming that it has uniform pattern i.e., Beta(1.0, 1.0) because all these instance (i.e., 0/13, 1/13, 2/13, 3/13, 4/13, …, 12/13 or even 13/13) have a equal chance of happening. This is an example of an uninformative prior We can code this information in Stan. FIRST: We specify the total number of student as integer int N which cannot be a negative number in the data block. Also, we also need to specify the number of students that passed as an integer int p which cannot be a negative number in the data block too. data { int&lt;lower = 0&gt; N; int&lt;lower = 0&gt; p; } SECOND: For the parameters block, here we will need to specify the name of the parameter that we want to infer. Here, its \\(\\theta\\) which is the proportion of those who got a distinction in GEOG0125. Note that \\(\\theta\\) follows a beta distribution and is therefore translated as a probability as a real value that is within the range of 0 to 1, inclusive. data { int&lt;lower = 0&gt; N; int&lt;lower = 0&gt; p; } parameters { real&lt;lower=0, upper=1&gt; theta; } THIRD: For the model block, here we will need to define our posterior distributions. Here, we need to state that p likelihood function is sampled from a binomial distribution as a function of N and \\(\\theta\\) (binomial(N, theta)). We also have to say \\(\\theta\\) is sampled from a beta distribution that is uniform. The model block will be: data { int&lt;lower = 0&gt; N; int&lt;lower = 0&gt; p; } parameters { real&lt;lower=0, upper=1&gt; theta; } model { p ~ binomial(N, theta); // our likelihood function or observation model theta ~ beta(1.0, 1.0); // our prior distribution alpha = 1 and beta = 1 } COMPLIMENTS: Well done, we have built our first Bayesian model. Let save it the script, what we need to do is compile and run it through RStudio to get our results. 2.6 Basic building blocks IV: Compiling our Stan code in RStudio Now, let us turn our attention to RStudio. The Stan script needs to be compiled from an R script. We will first need to create a list object using list() to connect the data to the information specified in the data before running the function stan() to the Bayesian model to get the results. The N and p will need to defined in the list object. # create list object and call it dataset dataset &lt;- list(N = 13, p = 4) Now, using the stan() to compile and obtain the posterior estimation of the proportions of students passing with a distinction: # the directory needs to be set to where you save the datasets and Stan script prediction.passes = stan(&quot;Predicting a proportion.stan&quot;, data=dataset, iter=3000, chains=3, verbose = FALSE) Some notes on the above code’s arguments: data= we are pushing the data we created from the list() to the Stan script. Stan is calling it. iter= we are asking the stan() to perform 3,000 iterations on each chain to generate the posterior samples. The chain can be a MCMC, NUTS or HMC algorithm (NUTS No-U-turn sampler is the default) chains= we are asking the stan() function to perform 3 chains (i.e., any of these algorithms can be stated in the function: i.e., MCMC, NUTS &amp; HMC) The resulting output can be printed with the function print(). Here, we are going to print the mean, standard error in mean, SD and the IQR ranges with 95% limits (i.e., 2.5% and 97.5%): print(predicting.passes, probs=c(0.025, 0.25, 0.5, 0.75, 0.975)) We obtain this summary table: Inference for Stan model: Predicting a proportion. 3 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=4500. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 0.33 0.00 0.12 0.12 0.24 0.32 0.41 0.58 1495 1 lp__ -10.08 0.02 0.73 -12.21 -10.26 -9.80 -9.61 -9.55 1879 1 Samples were drawn using NUTS(diag_e) at Thu Jan 12 14:39:10 2023. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). What does it all mean? The top part states that 3 chains were run for 3000 iterations. However, the first 1500 samples generated from each chain were discarded as warm-up, meaning that only 1500 samples from each chain were kept, resulting 4500 (1500x3) total post-warm-up sample draws. The output shows the summary statistics for our \\(\\theta\\). The lp__ is the log-probability - which is used to quantify how well the model is for the data but, in my opinion, its not a useful estimate. Instead, use the effective sample size n_eff and Rhat. If the Rhat is less than 1.1 for all parameters - it means that the estimation of our parameters are fine. 2.7 Basic building blocks V: Extract posterior samples and interpretation At this point, let us extract the posterior samples and graph them to understand it posterior distribution. We use the extract() function from the rstan package, and graph them: # extracting the samples (it should be 4500) theta.pass_draws &lt;- extract(predicting.passes, &#39;theta&#39;)[[1]] # create a graph i.e., histogram hist(theta.pass_draws, xlim = c(0,0.8), ylim = c(0,800), main= &quot;Posterior samples&quot;, ylab = &quot;Posterior Density&quot;, xlab = expression(paste(&quot;Probability of Parameter: &quot;, theta, &quot; [%]&quot;))) We want to compute the posterior mean, with the 0.025 and 0.975 quantiles (these limits are referred to as 95% credibility limits (95% CrI)). Here is how we compute them: # Calculating posterior mean (estimator) mean(theta.pass_draws) [1] 0.3305062 # Calculating posterior intervals quantile(theta.pass_draws, probs=c(0.025, 0.975)) 2.5% 97.5% 0.1239263 0.5800643 Interpretation: The predicted proportion of students passing the GEOG0125 from our sample posterior distribution was 0.3305 (33.05%). Our predictions, with 95% credibility, can be within the limits of 0.1239 and 0.5801. Formally writing as \\(\\theta\\) = 33.05% (95% CrI: 12.39-58.01%). 2.8 Basic building blocks VI: Updating our prediction with new information We can use the 0.3305 as our new prior as opposed to using a uniform again to make an updated prediction. The updated beta distribution takes a new form of Beta(5, 10) centered on 0.3305. This is an informative prior because we have information now. This was derived from multiplying the likelihood function (i.e., Binomial(13, 4)) with the prior (Beta(1,1)) results in a Beta distribution again. This is known as a conjugate. We can use this information in fact to make a prediction on the current 2022/23 cohort - hmmm, I wonder… how many of you will get a distinction in GEOG0125? Let’s see: Total sample size is now 26 (N) Assuming roughly a third (like last year) will probably get a distinction equates to 9 (p) Let us represent the proportion of distinction grades with some probability distribution \\(\\theta\\) with a Binomial distribution (likelihood function) [Binomial(26, 9)]. We have prior knowledge of last year’s proportion (0.3305) which has a Beta distribution [Beta(5, 10)] - this is our new informative prior Let updates the codes: Updated dataset in our RScript dataset.updated &lt;- list(N=26, p=9) Updated Stan script data { int&lt;lower = 0&gt; N; int&lt;lower = 0&gt; p; } parameters { real&lt;lower=0, upper=1&gt; theta; } model { p ~ binomial(N, theta); // our likelihood function or observation model theta ~ beta(5, 10); // our prior distribution updated to alpha = 5 and beta = 10 } Obtaining updated posterior samples by compiling new Stan script predicting.passes.updated = stan(&quot;Predicting a proportion updated.stan&quot;, data=dataset.updated, iter=3000, chains=3, verbose = FALSE) print(predicting.passes.updated, probs=c(0.025, 0.25, 0.5, 0.75, 0.975)) Updated output summary table Inference for Stan model: Predicting a proportion. 3 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=4500. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta 0.34 0.00 0.07 0.20 0.29 0.34 0.39 0.49 1956 1 lp__ -26.83 0.02 0.74 -28.94 -26.97 -26.54 -26.37 -26.32 1582 1 Samples were drawn using NUTS(diag_e) at Thu Jan 12 17:20:47 2023. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Histogram of posterior samples’ distribution # extracting the samples (it should be 4500) theta.pass_draws.new &lt;- extract(predicting.passes.updated, &#39;theta&#39;)[[1]] # create a graph i.e., histogram hist(theta.pass_draws.new, xlim = c(0,1), main= &quot;Posterior samples [Updated]&quot;, ylab = &quot;Posterior Density&quot;, xlab = expression(paste(&quot;Probability of Parameter: &quot;, theta, &quot; [%]&quot;))) Obtain updated posterior proportion and 95% CrI # Calculating posterior mean (estimator) mean(theta.pass_draws.new) [1] 0.3420825 # Calculating posterior intervals quantile(theta.pass_draws.new, probs=c(0.025, 0.975)) 2.5% 97.5% 0.2027673 0.4935101 Interpretation: For the current cohort, the predicted proportion of students passing the GEOG0125 from our updated posterior distribution was 0.3421 (34.21%). Our predictions, with 95% credibility, can be within the limits of 0.2027 and 0.4935. We can formally write this as \\(\\theta\\) = 34.21% (95% CrI: 20.27-49.35%). 2.9 Tasks 2.9.1 Task 1 - Low-level arsenic poisoning in Cornwall, UK Try this first problem in Stan: Suppose, in a small survey, a random sample of 50 villagers from a large population in Cornwall were at risk of arsenic poisoning due to long-term low-level environmental exposure status were selected. Each person’s disease status (i.e., metallic toxicity) was recorded as either Diseased or None. 19 of the respondents have found to be diseased. The distribution of the prevalence is assumed to be of a Beta distribution with Beta(3,5). What is the predicted posterior prevalence of arsenic poisoning in Cornwall and its 95% Credibility intervals? 2.9.2 Task 2 - Body mass index (BMI) problem Try this second problem in Stan: The mean BMI value is 23 with SD of 1.2. Simulate sample of 1000 with BMI values based on this distribution N(23, 1.2) and perform Bayesian inference. What is the posterior mean BMI and its 95% Credibility intervals? Hints: In the R script, use the function rnorm() to generate your sample of 1000 BMI points In the R script, create a list() with N and bmi defined In the Stan script, define the data block in accordance to the list() object In the Stan script, the bmi values are measured outcome. Code this in the model block as a likelihood function using the norm(mu, sigma) notation In the Stan script, use the parameter block, and make sure to code your mu (mean) and sigma (standard deviation) as real (non-negative) numbers Note: Solutions for task 1 and 2 will be made available later today "],["bayesian-generalised-linear-models-glms.html", "3 Bayesian Generalised Linear Models (GLMs) 3.1 Lecture recording (Length: 45:46 minutes) 3.2 Introduction 3.3 Poisson Regression Modelling 3.4 Logistic Regression Modelling 3.5 Tasks", " 3 Bayesian Generalised Linear Models (GLMs) 3.1 Lecture recording (Length: 45:46 minutes) Important Notes: If this or any subsequent video don’t play from this website, then try playing it directly from Microsoft Streams - you should be able to view it by clicking this [LINK]. Access to Microsoft Steams to these videos will require use of your UCL institutional login details. 3.2 Introduction This week we will learn how to perform Generalised Linear Modelling (GLMs) within a Bayesian framework. We will use quantitative data focused on crime victimization in Nigeria which have resulted in a number of interesting publications and see its application to a real study problem. Recall, the all forms of linear models only take continuous outcome as a dependent variable, this feature of a linear model can be limiting when have data following either a binary, count or categorical structure. A GLM typically consist of three components which allows a user to fit different dependent variable types: A probability distribution to specify the dependent variable’s conditional distribution; A linear predictor, which is the function of the predictors (or independent variables); A link function that connects point 1 to point 2. These models are often applied within a frequentist way of thinking; however, these are also easily estimated in a Bayesian setting. We going to take it a step further and show you how one can use Stan to encode various regression model types from a Bayesian perspective. We will specifically focus on models with likelihood functions that follows either a Binomial or Poisson distribution as they are used for risk assessments. 3.2.1 Learning outcomes Today’s session aims to formally introduce you to Stan programming for Bayesian regression models. By the end of this session, you should be able to perform the following: How to select the appropriate likelihood function specification for the Bayesian regression model i.e., binomial or Poisson to model either binary, or count outcomes respectively; How to fully develop Stan code for such regression models with the appropriate prior (i.e., uninformative, weak or informative) specification for various parameters; How to interpret the various types of coefficients e.g., Odds Ratios (OR) and Risk Ratios (RR); You can follow the live walkthrough demonstration delivered in the first 1-hour of the practical, and then use the remaining half to try the practical tutorials and follow the instructions on your own. 3.2.2 Demonstration recording (Length: 52:52 minutes) Important Notes: If this or any subsequent video don’t play from this website, then try playing it directly from Microsoft Streams - you should be able to view it by clicking this [LINK]. Access to Microsoft Steams to these videos will require use of your UCL institutional login details. 3.2.3 Datasets &amp; setting up the work directory Go to your folder GEOG0125 and create a sub folder called “Week 2” within the GEOG0125 folder. Here, we will store all our R &amp; Stan scripts as well as datasets. Set your work directory to Week 2’s folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0125/Week 2&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0125/Week 2&quot;) The dataset for this practical are: Street Burglary Data in Nigeria.csv Household victimisation data in Nigeria.csv London LSOA 2015 data.csv Context about the dataset: Conventional analyses of crime, based on European research models, are often poorly suited to assessing the specific dimensions of criminality in Africa. The data used in today’s practical is an anonymised excerpt from the Development Frontiers in Crime, Livelihoods and Urban Poverty in Nigeria (FCLP) project that aimed to provide an alternative framework for understanding the specific drivers of criminality in a West African urban context. This research project used a mixed-methods approach for combining statistical modeling, geovisualisation and ethnography, and attempted to situate insecurity and crime against a broader backdrop of rapid urban growth, seasonal migration, youth unemployment and informality. The study typically provided researchers both in Nigeria and internationally a richer and more nuanced evidence base on the particular dynamics of crime from an African perspective resulting a number of publications: [1], [2] and [3] . 3.2.4 Loading and installing packages We will need to load the following packages from previous practicals: rstan: a package that enables R to interface with Stan. It provides R the functions to code, parse, compile, test, estimate, and analyze Bayesian models through Stan in R. # Load the packages with library() library(&#39;rstan&#39;) Note that when you load rstan from cran you will see some recommendations on using multiple cores for speeding the process. For the best experience, we highly recommend using this code: options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) This tells RStudio to use multiple core for parallel process whenever Stan is being implemented. Every time you want to use Stan - make sure to load parallel::detectCores() and rstan_options code. 3.3 Poisson Regression Modelling We are going to fit a Poisson-type model on some an outcome that contain discrete counts of households reporting to have been victims of burglary. Let us load the data into RStudio can call the object burglaryDataset. # Set your own directory using setwd() function # Load data into RStudio using read.csv(). The spreadsheet is stored in the object called &#39;burglaryData&#39; burglaryDataset &lt;- read.csv(&quot;Street Burglary Data in Nigeria.csv&quot;) names(burglaryDataset) 3.3.1 Selecting the appropriate Poisson model There are three different types of Poisson models: Standard Poisson regression Negative Binomial Poisson regression Zero-Inflated Poisson regression The implementation of one of these models are highly dependent on how the frequency distribution of the count response variable are displayed. If it resembles a normal curve - then use the standard version. Otherwise, use the Negative Binomial Poisson regression if there is any evidence of over-dispersion. When there is a an inflation of zero counts in the dataset, you will have to use the Zero-Inflated model to account for the problem. Let’s check the frequency distribution of the outcome variable Burglary which corresponds to the number of reported instances a property on a street was burgled. You can simply use a histogram to examine its distribution: hist(burglaryDataset$Burglary, breaks=20) The plot show evidence of over-dispersion. It indicates that streets in Kaduna have less frequency of burglaries. Here, we consider using a Negative Binomial Poisson regression model over the standard and zero-inflated versions (i.e., scenario 1 and 3). Now, that we know the model type, let prepare the dataset of our independent variables. 3.3.2 Data preparation and set-up for Bayesian analysis in Stan Stan is able to handle continuous variable nicely; but for some strange reasons it’s absolutely terrible with categorical variables! It can only handle them as dummy variables - meaning that the categorical variable must be split into a series of binary variables. You will see what I mean in a second. The following variables: choice, integration, business activities and socioeconomic deprivation, at a street-level are examples of categorical variables which needs to be converted to dummy variables. They need to be make as a factor using the as.factor() function before implementing the model.matrix() to apply the conversion. Here is the code: # change the variable from numeric to a factor variable using the as.factor() function burglaryDataset$choice_q&lt;- as.factor(burglaryDataset$choice_q) burglaryDataset$integ_q &lt;- as.factor(burglaryDataset$integ_q) burglaryDataset$business_q &lt;- as.factor(burglaryDataset$business_q) burglaryDataset$socioeconomic_q &lt;- as.factor(burglaryDataset$socioeconomic_q) Next, we use the model.matrix() function to apply the conversion: # extract all the important independent variables to be used in model SelectedVariables &lt;- burglaryDataset[,c(4,5,6,7,8,9)] # convert only variable that are factor variables X &lt;- model.matrix(~ 0 + connectivity + choice_q + integ_q + business_q + socioeconomic_q, data = SelectedVariables) # see conversion View(X) Drop any column that is the first category - i.e., for instance choice_q1 is present. This needs to be dropped as its the referent category. # drop the column that has choice_q1 i.e., column three only X &lt;- X[,-3] Our independent variables are stored in the matrix X. Will use this specify this in the models when programming it in Stan. Now, we need to define our dependent variable y i.e., number of reported burglaries on a street, as well as the total numbers of houses on a street as an offset - these two quantities will also need to be specified in our Stan code. # declare the dependent variable y &lt;- burglaryDataset$burglary # declare totalhouse column as the denominators to be imputed as an offset in the model denominators &lt;- burglaryDataset$totalhouses Now, we need to condense this information into list() object as our data for Stan to read: stan.negbin.dataset &lt;- list(N=length(y), X=X, k=ncol(X), phi_scale=0.9, y=y, denominators=denominators) Important Notes: N = length(y) we are extracting the number of observations instead of fully typing the number. Here, N is 743 X=X: This is the set of independent variables we created earlier on with the dummy variables etc., It should contain 16 columns. k=ncol(X): We are extracting the number of columns from X phi_scale = 0.9: Here, we specify our over-dispersion parameter as 1 y=y: Here, we defined the outcome variable (i.e., counts of burglaries) as y denominators=denominators: Here, we define the totalhouse to be the denominators in the model At this point, the dataset is fully prepared are ready to go. Let’s create our Stan script for running a Negative Binomial Poisson regression within a Bayesian framework. 3.3.3 Creating a script to run a Negative Binomial Poisson regression in Stan A typical Stan program for a regression onsist of the following 5 blocks: Data Transformed data Parameters Transformed parameters Model Generated quantities The Data, Parameters and Model block must be specified for the regression to work. But there will be additional things that we need to transform such as the inputted data, parameters and our estimated coefficients for our relative risk (RR). Let’s start with the data block: FIRST STEP: We specify the total number of observations int N as well as the number of independent variables int k as an integer in the data block. We define our y as a vector of size N holding all the values for our dependent variable. We also define our independent variables a matrix X with N rows and k columns (i.e, matrix[N, k] X). Lastly, we define our overdispersion parameter as a real number real and call it phi_scale. We need to define the denominators as a vector: data { int&lt;lower = 1&gt; N; // Number of observations int&lt;lower = 1&gt; k; // Number of columns in X matrix contain independent variables and intercept column int y[N]; // Dependent variable - Number of reported burglaries on a street segment matrix[N,k] X; // Model matrix for our independent variables real phi_scale; vector&lt;lower = 1&gt;[N] denominators; } Next we need to apply a log() transformation on to the denominators to change it as an offset for the model. Here, we are making a direct change to the data; hence, we can use the transformed data block: data { int&lt;lower = 1&gt; N; // Number of observations int&lt;lower = 1&gt; k; // Number of columns in X matrix contain independent variables and intercept column int y[N]; // Dependent variable - Number of reported burglaries on a street segment matrix[N,k] X; // Model matrix for our independent variables real phi_scale; vector&lt;lower = 1&gt;[N] denominators; } transformed data { vector[N] offset = log(denominators); } SECOND STEP: For the parameters block, here we will need to specify the name of the regression intercept alpha and the coefficients for all our independent variables. Here, we call defined it as a vector of size k. The reciporcal_phi is a precision estimate which we will need as a prior in our model. data { int&lt;lower = 1&gt; N; // Number of observations int&lt;lower = 1&gt; k; // Number of columns in X matrix contain independent variables and intercept column int y[N]; // Dependent variable - Number of reported burglaries on a street segment matrix[N,k] X; // Model matrix for our independent variables real phi_scale; vector&lt;lower = 1&gt;[N] denominators; } transformed data { vector[N] offset = log(denominators); } parameters { real alpha; // intercept i.e., overall risk of burglary in population vector[k] beta; // regression coefficients real reciporcal_phi; // overdispersion } THIRD STEP: We define our regression model in the transformed parameters block. mu represents the predicted number of burglaries, which is a vector of N size (i.e., each street). The alpha + beta*X is the regression model. phi is the over-dispersion parameter accounted for in the model: data { int&lt;lower = 1&gt; N; // Number of observations int&lt;lower = 1&gt; k; // Number of columns in X matrix contain independent variables and intercept column int y[N]; // Dependent variable - Number of reported burglaries on a street segment matrix[N,k] X; // Model matrix for our independent variables real phi_scale; vector&lt;lower = 1&gt;[N] denominators; } transformed data { vector[N] offset = log(denominators); } parameters { real alpha; // intercept i.e., overall risk of burglary in population vector[k] beta; // regression coefficients real reciporcal_phi; // overdispersion } transformed parameters { vector[N] mu; // create mu our estimator for model real phi; // define precision parameter mu = offset + alpha + X*beta; // linear predictor for mu i.e., predicted burden of burglaries phi = 1./reciporcal_phi; // precision (akin to variation) } FOURTH STEP: We build our likelihood function and specify the priors for each parameter under the model block: data { int&lt;lower = 1&gt; N; // Number of observations int&lt;lower = 1&gt; k; // Number of columns in X matrix contain independent variables and intercept column int y[N]; // Dependent variable - Number of reported burglaries on a street segment matrix[N,k] X; // Model matrix for our independent variables real phi_scale; vector&lt;lower = 1&gt;[N] denominators; } transformed data { vector[N] offset = log(denominators); } parameters { real alpha; // intercept i.e., overall risk of burglary in population vector[k] beta; // regression coefficients real reciporcal_phi; // overdispersion } transformed parameters { vector[N] mu; // create mu our estimator for model real phi; // define precision parameter mu = offset + alpha + X*beta; // linear predictor for mu i.e., predicted burden of burglaries phi = 1./reciporcal_phi; // precision (akin to variation) } model { reciporcal_phi ~ cauchy(0, phi_scale); // prior for reciporcal_phi using half-cauchy alpha ~ normal(0, 10); // prior for intercept i.e., normal with mean 0 and ±10 variance beta ~ normal(0, 2.5); // prior for beta i.e., normal with mean 0 and ±2.5 variance y ~ neg_binomial_2_log(mu, phi); // likelihood function for burglaries neg_binomial_2_log() } LAST STEP: We instruct Stan on the parameters we want to report. We want them a relative risk ratio (RR) or should we say a crime risk ratio (CRR). We use the generated quantities block to obtain the estimates we need i.e., exponentiation pf the intercept and coefficients: data { int&lt;lower = 1&gt; N; // Number of observations int&lt;lower = 1&gt; k; // Number of columns in X matrix contain independent variables and intercept column int y[N]; // Dependent variable - Number of reported burglaries on a street segment matrix[N,k] X; // Model matrix for our independent variables real phi_scale; vector&lt;lower = 1&gt;[N] denominators; } transformed data { vector[N] offset = log(denominators); } parameters { real alpha; // intercept i.e., overall risk of burglary in population vector[k] beta; // regression coefficients real reciporcal_phi; // overdispersion } transformed parameters { vector[N] mu; // create mu our estimator for model real phi; // define precision parameter mu = offset + alpha + X*beta; // linear predictor for mu i.e., predicted burden of burglaries phi = 1./reciporcal_phi; // precision (akin to variation) } model { reciporcal_phi ~ cauchy(0, phi_scale); // prior for reciporcal_phi using half-cauchy alpha ~ normal(0, 10); // prior for intercept i.e., normal with mean 0 and ±10 variance beta ~ normal(0, 2.5); // prior for beta i.e., normal with mean 0 and ±2.5 variance y ~ neg_binomial_2_log(mu, phi); // likelihood function for burglaries neg_binomial_2_log() } generated quantities { vector[N] eta; // vector containing our predicted number of burglaries for each street vector[k] crr; // vector containing our estimated crime risk ratios for each variable real alpha_crr; alpha_crr = exp(alpha); eta = exp(mu); // overall risk ratio crr = exp(beta); // risk ratio for each variable } COMPLIMENTS: Well done - you are doing well. You have coded your first Bayesian regression. Let’s save the script, and then compile and run it through RStudio to get our crime risk ratio (CRR) results. 3.3.4 Compiling our Stan code in RStudio Now, let us turn our attention to RStudio. Using the stan() to compile and obtain the posterior estimation of the overall risk and crime risk ratios (CRR) for the each independent variable: # the directory needs to be set to where you saved the dataset and Stan script crr.negbin.model = stan(&quot;Script for NegBin model.stan&quot;, data=stan.negbin.dataset, iter=5000, chains=3, verbose = FALSE) # take roughly 1-2 minutes to compile We can print the results accordingly: # reports the crime rate ratio (relative risk) print(crr.negbin.model, probs=c(0.025, 0.975), pars = c(&quot;alpha_crr&quot;, &quot;crr&quot;, &quot;lp__&quot;)) # take roughly 1-2 minutes to compile Output summary table Inference for Stan model: anon_model. 3 chains, each with iter=5000; warmup=2500; thin=1; post-warmup draws per chain=2500, total post-warmup draws=7500. mean se_mean sd 2.5% 97.5% n_eff Rhat alpha_crr 0.12 0.00 0.04 0.07 0.21 3289 1 crr[1] 1.00 0.00 0.00 1.00 1.00 10232 1 crr[2] 1.07 0.00 0.03 1.01 1.13 5844 1 crr[3] 0.96 0.00 0.21 0.60 1.44 5245 1 crr[4] 1.03 0.00 0.25 0.63 1.60 4469 1 crr[5] 0.85 0.00 0.28 0.44 1.52 4323 1 crr[6] 0.99 0.00 0.22 0.64 1.48 5065 1 crr[7] 0.90 0.00 0.20 0.57 1.36 4608 1 crr[8] 0.85 0.00 0.21 0.51 1.34 4607 1 crr[9] 0.93 0.00 0.25 0.53 1.50 4888 1 crr[10] 0.70 0.00 0.17 0.42 1.10 4924 1 crr[11] 1.15 0.00 0.27 0.71 1.78 4700 1 crr[12] 1.21 0.00 0.29 0.73 1.88 4495 1 crr[13] 1.23 0.00 0.29 0.76 1.88 3971 1 crr[14] 0.95 0.00 0.22 0.59 1.46 4427 1 crr[15] 0.95 0.00 0.24 0.56 1.49 4366 1 crr[16] 1.25 0.01 0.32 0.74 1.98 4097 1 lp__ -1023.04 0.06 3.03 -1029.81 -1018.04 2670 1 Samples were drawn using NUTS(diag_e) at Thu Jan 19 22:33:08 2023. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Here is the messing part - interpretation. Before anything, note that alpha_crr corresponds to the intercept. Each crr[1], crr[2],… and so on corresponds to the ordering of the columns’ in X matrix containing the independent variables. If you run this code of colnames(X) - it will print this list of names: colnames(X) Output [1] &quot;distance&quot; &quot;connectivity&quot; &quot;choice_q2&quot; &quot;choice_q3&quot; &quot;choice_q4&quot; [6] &quot;integ_q2&quot; &quot;integ_q3&quot; &quot;integ_q4&quot; &quot;business_q2&quot; &quot;business_q3&quot; [11] &quot;business_q4&quot; &quot;business_q5&quot; &quot;socioeconomic_q2&quot; &quot;socioeconomic_q3&quot; &quot;socioeconomic_q4&quot; [16] &quot;socioeconomic_q5&quot; alpha_crr is the risk ratio for the intercept crr[1] corresponds to the continuous distance variable crr[2] corresponds to the continuous connectivity variable crr[3], crr[4] and crr[5] corresponds to the 2nd, 3rd, 4th categories for choice variable crr[6], crr[7] and crr[8] corresponds to the 2nd, 3rd, 4th categories for integration variable crr[9], crr[10], crr[11] and crr[12] corresponds to the 2nd, 3rd, 4th and 5th categories for business activities variable crr[13], crr[14], crr[15] and crr[16] corresponds to the 2nd, 3rd, 4th and 5th categories for socioeconomic deprivation variable Interpretation of interesting coefficients: Connectivity (cont.): For a unit increase in the number of connections on a street segment significantly increases the risk of burglaries by 7% (CRR 1.07, 95% CrI: 1.01–1.13). or you can say the risk for burglaries in relation to a unit increase in the number of connections on a street are 1.07 times higher. Business activities (cat.): Relative to street segments will lower levels of business activities, streets that are in the highest quintiles (i.e., higher levels of commerce) are more likely to report burglaries (4th category: CRR 1.15, 95% CrI: 0.71-1.78; 5th category: CRR 1.21, 95% CrI: 0.73-1.88) 3.4 Logistic Regression Modelling 3.4.1 Data preparation and set-up for Bayesian analysis in Stan Running a logistic regression is very easy. It has household-level information regarding the security status of a property and victimisation (i.e., victim of burglary). Here are the list of variables: Gate: Is the property gated? (1 = yes, 0 = no) Garage: Does the property contains a garage? (1 = yes, 0 = no) Outdoor_sitting: Does property have an outdoor sitting? (1 = yes, 0 = no) Security_lights: Are security lights attached to property? (1 = yes, 0 = no) BurglaryStatus: Whether the household has been a victim of burglary (1 = yes, 0 = no) - outcome variable. Let’s load this dataset i.e., Household victimisation data in Nigeria.csv and attempt to implement a logistic regression model using the Bernoulli likelihood function on a binary dependent variable that is BurglaryStatus to assess the impacts of properties protected by some form of security. Let’s get straight to the preparing the dataset for Stan: # load data houselevel.data &lt;- read.csv(&quot;Household victimisation data in Nigeria.csv&quot;) # declare binary variables as factor variables houselevel.data$Gate&lt;- as.factor(houselevel.data$Gate) houselevel.data$Garage &lt;- as.factor(houselevel.data$Garage) houselevel.data$Outdoor_sitting &lt;- as.factor(houselevel.data$Outdoor_sitting) houselevel.data$Security_lights &lt;- as.factor(houselevel.data$Security_lights) # create a matrix for the independent variables X &lt;- model.matrix(~ 0 + Gate + Garage + Outdoor_sitting + Security_lights, data = houselevel.data) # declare the dependent variable y &lt;- houselevel.data$BurglaryStatus # compile all information into list() object for Stan to compile in logistic regression script stan.logistic.dataset &lt;- list(N=length(y), X=X, k=ncol(X), y=y) 3.4.2 Creating a script to run a logistic regression in Stan Let’s get straight to writing the logistic regression script in Stan. // Stan script for running logistic regression model data { int&lt;lower = 1&gt; N; int&lt;lower = 1&gt; k; int&lt;lower = 0, upper = 1&gt; y [N]; // Burglary status 1 = yes, 0 = no matrix[N, k] X; } parameters { real alpha; vector[k] beta; } model { y ~ bernoulli_logit(alpha + X*beta); // fit a logistic regression using a bernoulli_logit() alpha ~ normal(0, 10); beta ~ normal(0, 2.5); } generated quantities { vector[k] orr; real alpha_orr; alpha_orr = exp(alpha); orr = exp(beta); } Now, save the script with an appropriate name, and we can compile the code in RStudio with stan() function: orr.logistic.model = stan(&quot;Script for logit model.stan&quot;, data=stan.logistic.dataset, iter=5000, chains=3, verbose = FALSE) Print the results for risk of burglaries in relation to target hardening security features: # reports the odds ratios (OR) print(orr.logistic.model, probs=c(0.025, 0.975), pars = c(&quot;alpha_orr&quot;, &quot;orr&quot;, &quot;lp__&quot;)) Output summary table Inference for Stan model: anon_model. 3 chains, each with iter=5000; warmup=2500; thin=1; post-warmup draws per chain=2500, total post-warmup draws=7500. mean se_mean sd 2.5% 97.5% n_eff Rhat alpha_orr 0.16 0.00 0.01 0.13 0.19 4591 1 orr[1] 1.06 0.00 0.12 0.85 1.31 5589 1 orr[2] 1.04 0.00 0.23 0.64 1.56 6729 1 orr[3] 1.08 0.00 0.13 0.85 1.35 6637 1 orr[4] 1.22 0.00 0.13 0.99 1.49 5937 1 lp__ -1268.99 0.03 1.60 -1272.95 -1266.93 3375 1 Samples were drawn using NUTS(diag_e) at Fri Jan 20 00:35:46 2023. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Here is the messing part again - interpretation. Before anything, note that alpha_orr corresponds to the intercept. Each orr[1], orr[2],… and so on corresponds to the ordering of the columns’ in X matrix containing the independent variables. If you runcolnames(X) it will print this list: colnames(X) Printing list of variable names in X matrix [1] &quot;Gate&quot; &quot;Garage&quot; &quot;Outdoor_sitting&quot; &quot;Security_lights&quot; alpha_orr is the odds ratio for the intercept orr[1] corresponds to the category in Gate = 1 that means it has a gate orr[2] corresponds to the category in Garage = 1 that means it has a garage orr[3] corresponds to the category in Outdoor_sitting = 1 that means it has a outdoor settings to property orr[4] corresponds to the category in Security lights = 1 that means it has a security lights outside its property Interpretation of interesting coefficients: Gated properties (cat.): Compared to properties without gates, those with this security feature are targeted and hence 6% more likely to be a victim of burglary (OR: 1.06, 95% CrI 0.85-1.31). Properties with Garages (cat.): Compared to properties without garages, those with this feature are targeted and hence 4% more likely to be a victim of burglary (OR: 1.04, 95% CrI 0.64-1.56). Properties with Outdoor sittings (cat.): Compared to properties with no outdoor features, those with this feature are targeted and hence 8% more likely to be a victim of burglary (OR: 1.08, 95% CrI 0.85-1.35). Properties with Security lightings (cat.): Compared to properties with no secruity lights, those with this feature are targeted and hence 22% more likely to be a victim of burglary (OR: 1.22, 95% CrI 0.99-1.49). 3.5 Tasks 3.5.1 Task 1 - Obesity and Fastfoods in London Accessibility to junk food restaurants in young adolescents especially after school hours is a growing cause for concern. Especially, now that many young adults have a sedentary lifestyle; hence obesity rates among this population is increasing in the UK. Try this problem in Stan: Use the dataset Obesity and Fastfoods in MSOAs data.csv to determine the links between prevalence of obesity in high school students and density of fast food (cheap) restaurant and deprivation in MSOAs in London. Implement a Bayesian GLM using Stan code. Variable names: SEQID: ID number for row MSOA11CD: Unique identifier for MSOA area MSOA11NM: Name of the MSOA area OBESE: Number of child identified as obese in MSOA in London TOTAL: Total number of children surveyed for BMI measurements IMDMSOA: Area-level socioeconomic deprivation score (higher scores means higher deprivation and vice versa) RESTCAT: Categorical variable for classifying an MSOA in terms of density of junk/cheap fast food outlets restaurants: 1 = 1 to 10, 2= 11 to 25, 3= 26 to 50 and 4= 51 to 300. HINT: You might want to consider using the following functions: binomial_logit() or binomial() in the model block and reporting the odd ratios using the generated quantities block. Also, use the as.factor() for independent variables that are categorical. You can put all independent variables into a matrix using the code using the model.matrix() function. You might want to consider computing the exceedance probabilities for the odd ratios using the threshold of 1. 3.5.2 Task 2 - Factors affecting house prices in London (2015) Try this problem in Stan: Use London LSOA 2015 data.csv data pertained house prices in 2015, and assess it’s relationship with public transport accessibility (PTA), average income and socioeconomic deprivation (IMD) as the independent variables. Implement a Bayesian GLM using Stan code. Variables names: LSOACODE: Unique identification code for the geographic area AVEPRICE: (Dependent variable) Average house price estimated for the LSOA in 2015 AVEINCOME: Estimated average annual income for households within an LSOA in 2015 IMDSCORE: Deprivation score for an LSOA in 2015 PTAINDEX: Measures levels of access/connectivity to public transport HINT: You might want to consider using the following functions: normal() in the model block. You might want to consider computing the exceedance probabilities for the coefficients using the threshold of 0. Note: Solutions will be made available later today. "],["bayesian-generalised-additive-models-gams.html", "4 Bayesian Generalised Additive Models (GAMs) 4.1 Lecture recording (Length: 38:12 minutes) 4.2 Introduction 4.3 Implementing a Bayesian Generalised Additive Model 4.4 Printing Stan code from brms::stancode()", " 4 Bayesian Generalised Additive Models (GAMs) 4.1 Lecture recording (Length: 38:12 minutes) Important Notes: If this or any subsequent video don’t play from this website, then try playing it directly from Microsoft Streams - you should be able to view it by clicking this [LINK]. Access to Microsoft Steams to these videos will require use of your UCL institutional login details. 4.2 Introduction To date, we have been using various models that considers the relationship between variables to be assumed as something that’s linear. When implementing a regression of any kind, we are attempting to fit a straight line (i.e., line of best fit) to the data in order to examine and explain variations between two or more variables with some outcome with the strong assumption of linearity. The issue of linearity may not be case (which often is not the case) and hence there must be some model that can model non-linear relationships. This is where Generalised Additive Models (GAMs) come to play - this week we will learn how to use GAMs, within a Bayesian framework. This approach is incredibly easy to perform in RStudio as there are useful packages that generates the Stan code for GAMs. Let us begin! 4.2.1 Datasets &amp; setting up the work directory Go to your folder GEOG0125 and create a sub folder called “Week 3” within the GEOG0125 folder. Here, we will store all our R &amp; Stan scripts as well as datasets. Set your work directory to Week 3’s folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0125/Week 3&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0125/Week 3&quot;) The dataset for this practical are: Respiratory Illness in Turin Province.csv Now, load the dataset: # load up the data respiratory_data &lt;- read.csv(&quot;Respiratory Illness in Turin Province.csv&quot;) It contains the information taught in today’s lecture. We will reproduce the analysis. 4.2.2 Loading and installing packages Install the following packages for today’s practicals. # install the following packages install.packages(&quot;brms&quot;) install.packages(&quot;mgcv&quot;) install.packages(&quot;devtools&quot;) install.packages(&quot;ggplot2&quot;) devtools::install_github(&#39;gavinsimpson/schoenberg&#39;) Now load the installed packages including rstan: # load up packages library(&quot;brms&quot;) library(&quot;mgcv&quot;) library(&quot;ggplot2&quot;) library(&quot;schoenberg&quot;) library(&quot;rstan&quot;) Make sure to select the appropriate number of cores on your computer - as this will preset the number of chains for the analysis. parallel::detectCores() On my computer, it is six. Make sure to take note on what you number of cores are. Now the usual, configure Stan: # configure Stan options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) 4.3 Implementing a Bayesian Generalised Additive Model Coding a GAM model is extremely hard but lucky - we have the brms package which stands for Bayesian Regression Modelling in Stan. It has a very streamlined function brm() for executing a Bayesian model by calling Stan without the need to actually it in Stan, making it very easy. In fact, it even helps us to determine prior distributions for the parameters we want to estimate from our GAM model. All we need is to write the formula in the get_prior(): # get prior prior.list &lt;- get_prior(Overall ~ s(PM10) + s(CO) + s(NO2), data = respiratory_data, family = poisson()) This equation: Overall ~ s(PM10) + s(CO) + s(NO2) is our statistical model which follows a poisson() function. Notice the s() is wrapped round our independent variables. That is how we specify a smoother on an variable. In this example, we are applying smoothers to all independent variables. We can run a GAM using the brms(). It like running the ls(). # start timer gauge how long it takes to run a GAM ptm &lt;- proc.time() # run a GAM model model.bayes.gam &lt;- brm(bf(Overall ~ s(PM10) + s(CO) + s(NO2)), data = respiratory_data, family = poisson(), prior = prior.list, cores = 6, # replace this with number of cores on your own machine iter = 8000, warmup = 1000, thin = 10, refresh = 0, control = list(adapt_delta = 0.99)) # Stop the clock proc.time() - ptm It takes approximately 9 mins to complete the computation of the sample posterior. We can visualise the results using the summary() function: # Report results summary(model.bayes.gam) Summary Output [See slides 24] Family: poisson Links: mu = log Formula: Overall ~ s(PM10) + s(CO) + s(NO2) Data: respiratory_data (Number of observations: 315) Draws: 4 chains, each with iter = 8000; warmup = 1000; thin = 10; total post-warmup draws = 2800 Smooth Terms: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sds(sPM10_1) 6.57 1.91 3.92 11.63 1.01 400 827 sds(sCO_1) 5.83 1.60 3.72 9.77 1.01 295 551 sds(sNO2_1) 6.30 1.60 4.00 10.03 1.01 430 1016 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 3.56 0.01 3.54 3.59 1.00 1565 2241 sPM10_1 -19.46 3.40 -26.25 -12.94 1.01 445 906 sCO_1 33.75 3.17 27.43 39.90 1.01 396 804 sNO2_1 4.35 4.96 -5.43 14.39 1.01 259 455 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). The result under the Smooth Terms are variance parameters, which describes the extent of “wiggliness” of the smooth — the larger the value the more wigglier the smooth, and vice versa. We can see that the credible interval doesn’t include 0 so there is evidence that a smooth is required over and above a linear. If it did exclude the null value of zero - then it would mean there’s no need to apply a smoother on that variable. Here, it was correct for us to apply a GAM model on these three variables. Also, the model is valid since the Rhat estimates that are below 1.05 The result under the Population-level effects are simply global estimates which are considered as fixed effects. We will interpret these as we usually interpret a regression the usual way. The above table essentially produces overall results. But we want the coefficient-specific estimate for each value measured in our independent variable. We can extract this result from model.bayes.gam object using the conditional_smooths() function. smooth_plot &lt;- conditional_smooths(model.bayes.gam) Run the following code in console: smooth_plot$`mu: s(PM10)` This should show the posterior specific estimates for PM10: PM10 effect1__ cond__ estimate__ se__ lower__ upper__ 1 17.63150 17.63150 1 2.62097348 0.58419108 1.4172802112 3.78413878 2 19.24250 19.24250 1 2.57521132 0.56128002 1.4271478599 3.67997666 3 20.85350 20.85350 1 2.53090840 0.54322413 1.4125906851 3.59283584 4 22.46449 22.46449 1 2.48331531 0.52194251 1.4022329526 3.51408796 5 24.07549 24.07549 1 2.44123786 0.50497874 1.3973972175 3.43957333 6 25.68649 25.68649 1 2.40463959 0.48327388 1.3978011917 3.36643850 7 27.29749 27.29749 1 2.37041186 0.47086036 1.3958640356 3.28785824 8 28.90848 28.90848 1 2.34151285 0.45395873 1.4096217561 3.21844181 9 30.51948 30.51948 1 2.32250696 0.43903983 1.4396053597 3.17590372 10 32.13048 32.13048 1 2.31045468 0.42353149 1.4655908589 3.13094097 11 33.74148 33.74148 1 2.30418617 0.40966546 1.4872374125 3.09568692 12 35.35247 35.35247 1 2.30994093 0.39628829 1.5180691767 3.07073513 13 36.96347 36.96347 1 2.31814000 0.38190394 1.5579488275 3.04882364 14 38.57447 38.57447 1 2.33250802 0.37203591 1.6000727952 3.03712395 15 40.18547 40.18547 1 2.35002277 0.35667034 1.6531294642 3.02495744 16 41.79646 41.79646 1 2.36561240 0.33989502 1.7079949755 3.01680741 17 43.40746 43.40746 1 2.38308174 0.32717131 1.7585588559 3.00095344 18 45.01846 45.01846 1 2.40230261 0.31269355 1.7996081811 2.98990205 19 46.62946 46.62946 1 2.41608040 0.29657785 1.8450409374 2.97535575 20 48.24045 48.24045 1 2.42229899 0.27875546 1.8825763877 2.95464690 21 49.85145 49.85145 1 2.42185750 0.26111552 1.9082253754 2.92646095 22 51.46245 51.46245 1 2.41606038 0.24644429 1.9248348118 2.89337966 23 53.07345 53.07345 1 2.39506092 0.23390482 1.9218254499 2.85807943 24 54.68444 54.68444 1 2.36402488 0.22431384 1.9093917777 2.81362396 25 56.29544 56.29544 1 2.32589212 0.21393800 1.8852152209 2.75454643 26 57.90644 57.90644 1 2.28190391 0.20780909 1.8571817966 2.69251749 27 59.51744 59.51744 1 2.23039023 0.19945464 1.8244201954 2.62279135 28 61.12843 61.12843 1 2.17075723 0.18810909 1.7807092880 2.55284812 29 62.73943 62.73943 1 2.10757795 0.18250459 1.7312261291 2.47443043 30 64.35043 64.35043 1 2.04432131 0.17743278 1.6808230354 2.40017233 31 65.96143 65.96143 1 1.97719004 0.17271403 1.6317107455 2.32570753 32 67.57242 67.57242 1 1.91449416 0.16605060 1.5832596809 2.24791409 33 69.18342 69.18342 1 1.85511399 0.16055720 1.5309816170 2.18115654 34 70.79442 70.79442 1 1.80101939 0.15951536 1.4875723704 2.11322875 35 72.40542 72.40542 1 1.75231598 0.15434167 1.4482471767 2.05785024 36 74.01641 74.01641 1 1.71355011 0.14961617 1.4179592295 2.00578986 37 75.62741 75.62741 1 1.68147066 0.14388785 1.3995074794 1.96245797 38 77.23841 77.23841 1 1.65967140 0.13853479 1.3882695320 1.93246780 39 78.84941 78.84941 1 1.64791200 0.13459759 1.3858286175 1.90978707 40 80.46040 80.46040 1 1.64336048 0.12875041 1.3908758754 1.89325457 41 82.07140 82.07140 1 1.64492013 0.12268128 1.4031639860 1.88175612 42 83.68240 83.68240 1 1.65417851 0.11713959 1.4235300062 1.87808877 43 85.29340 85.29340 1 1.66470527 0.11034031 1.4449902794 1.87569038 44 86.90439 86.90439 1 1.67664542 0.10498627 1.4690518449 1.87324834 45 88.51539 88.51539 1 1.68465372 0.10091327 1.4884726332 1.87169557 46 90.12639 90.12639 1 1.68897643 0.09330719 1.5035928627 1.87098835 47 91.73739 91.73739 1 1.68347289 0.08683769 1.5096123619 1.85667260 48 93.34838 93.34838 1 1.67060921 0.08271990 1.5076451208 1.83537867 49 94.95938 94.95938 1 1.64464120 0.07771426 1.4903394988 1.80144760 50 96.57038 96.57038 1 1.60848980 0.07453532 1.4585948072 1.76077187 51 98.18138 98.18138 1 1.56015919 0.07375575 1.4156532522 1.70600263 52 99.79237 99.79237 1 1.49690657 0.07238988 1.3603992597 1.64393361 53 101.40337 101.40337 1 1.42440125 0.07373672 1.2907121958 1.57108031 54 103.01437 103.01437 1 1.34150729 0.07264463 1.2096780201 1.49012333 55 104.62537 104.62537 1 1.24797093 0.07222848 1.1135088788 1.39686834 56 106.23636 106.23636 1 1.14710058 0.07207578 1.0121126540 1.29687463 57 107.84736 107.84736 1 1.04280793 0.07222129 0.9058132254 1.19160851 58 109.45836 109.45836 1 0.93526416 0.07258752 0.7972667841 1.08526921 59 111.06936 111.06936 1 0.82787351 0.07342898 0.6888869747 0.97590446 60 112.68035 112.68035 1 0.72334638 0.07249049 0.5850385387 0.87003656 61 114.29135 114.29135 1 0.62353761 0.07184606 0.4844786298 0.76856511 62 115.90235 115.90235 1 0.52968239 0.07181101 0.3905574540 0.67239135 63 117.51335 117.51335 1 0.44344659 0.07074263 0.3038719320 0.58689049 64 119.12434 119.12434 1 0.36603452 0.07036476 0.2283629332 0.50649586 65 120.73534 120.73534 1 0.29648469 0.06876687 0.1594488329 0.43632890 66 122.34634 122.34634 1 0.23554247 0.06794464 0.1006475466 0.37354216 67 123.95734 123.95734 1 0.18000095 0.06758691 0.0497623523 0.31633100 68 125.56833 125.56833 1 0.13053101 0.06618215 0.0007378314 0.26424762 69 127.17933 127.17933 1 0.08206403 0.06474997 -0.0467437618 0.21423914 70 128.79033 128.79033 1 0.03134801 0.06480924 -0.0974891106 0.16225348 71 130.40133 130.40133 1 -0.02507498 0.06446464 -0.1533213958 0.10484075 72 132.01232 132.01232 1 -0.08949573 0.06458059 -0.2185494998 0.04043094 73 133.62332 133.62332 1 -0.16544886 0.06520103 -0.2941522813 -0.03573511 74 135.23432 135.23432 1 -0.25395271 0.06544914 -0.3823353866 -0.12530644 75 136.84532 136.84532 1 -0.35452839 0.06517862 -0.4834635198 -0.22497655 76 138.45631 138.45631 1 -0.46556823 0.06511143 -0.5947961412 -0.33433084 77 140.06731 140.06731 1 -0.58572483 0.06552576 -0.7145562107 -0.45392189 78 141.67831 141.67831 1 -0.71021821 0.06659227 -0.8374210177 -0.57922044 79 143.28931 143.28931 1 -0.83427326 0.06712725 -0.9628190317 -0.70353029 80 144.90030 144.90030 1 -0.95489071 0.06664170 -1.0866594998 -0.82278030 81 146.51130 146.51130 1 -1.06836138 0.06723713 -1.2009492109 -0.93596858 82 148.12230 148.12230 1 -1.17158620 0.06741626 -1.3056400369 -1.03885479 83 149.73330 149.73330 1 -1.26424326 0.06818362 -1.3967667804 -1.13022022 84 151.34429 151.34429 1 -1.34683285 0.06863472 -1.4788035367 -1.21287485 85 152.95529 152.95529 1 -1.42127536 0.06919206 -1.5539866490 -1.28840211 86 154.56629 154.56629 1 -1.49103166 0.06843476 -1.6244977766 -1.35811880 87 156.17729 156.17729 1 -1.55861848 0.06932546 -1.6943199126 -1.42259813 88 157.78828 157.78828 1 -1.62686300 0.06936522 -1.7642742006 -1.49149207 89 159.39928 159.39928 1 -1.69919472 0.06909321 -1.8378600178 -1.56336851 90 161.01028 161.01028 1 -1.77983166 0.06959826 -1.9187291284 -1.64353354 91 162.62128 162.62128 1 -1.86963932 0.06948871 -2.0093699789 -1.73373903 92 164.23228 164.23228 1 -1.96957576 0.07001212 -2.1119305781 -1.83262600 93 165.84327 165.84327 1 -2.08094069 0.07075931 -2.2247677724 -1.94291702 94 167.45427 167.45427 1 -2.20301138 0.07164868 -2.3493314976 -2.06646808 95 169.06527 169.06527 1 -2.33352869 0.07203991 -2.4810339237 -2.19639859 96 170.67627 170.67627 1 -2.47199154 0.07320017 -2.6211551967 -2.33402310 97 172.28726 172.28726 1 -2.61514938 0.07559344 -2.7664659941 -2.47323472 98 173.89826 173.89826 1 -2.76218030 0.07684584 -2.9162540724 -2.61667948 99 175.50926 175.50926 1 -2.91141448 0.08046156 -3.0699949230 -2.76090593 100 177.12026 177.12026 1 -3.06126468 0.08357851 -3.2261306757 -2.90331811 We will extract the column 1, 4, 6 and 7 in order to graph the coefficients against the PM10. This essentially tells us the impact of PM10 on admissions for all values of PM10. Column is the observed sampled PM10, column 4 is our coefficients estimated from our basis function (see lecture), and column 6 and 7 is our lower and upper credibility intervals. Here is the code for extracting the columns for the model.bayes.gam object: # create a data frame beta_coef_PM10 &lt;- data.frame(pm10 = smooth_plot$`mu: s(PM10)`[1], beta = smooth_plot$`mu: s(PM10)`[4], lower = smooth_plot$`mu: s(PM10)`[6], lower = smooth_plot$`mu: s(PM10)`[7]) Now, we will plot our final output for PM10 and its impact on respiratory admission: # generate a plot with coefficients and PM10 plot(beta_coef_PM10$PM10, beta_coef_PM10$estimate__, type=&quot;l&quot;, ylab = &quot;Coefficient for PM10&quot;, xlab=&quot;Observed PM10&quot;, ylim=c(-4, 5), main = &quot;GAM: Impact Respiratory burden in Turin&quot;) # Add lines for 95% credibility limits to above graph lines(beta_coef_PM10$PM10, beta_coef_PM10$upper__,col=&quot;grey&quot;) lines(beta_coef_PM10$PM10, beta_coef_PM10$lower__,col=&quot;grey&quot;) # Add null value line at origin abline(h=0, lty = &quot;dashed&quot;, col =&quot;red&quot;) Figure output Interpretation: The overall impact of PM10 on respiratory-related admissions show a significant decreasing trend across it’s values (-19.46, 95% CrI: -26.25 to -12.94). However, it should be noted that PM10 levels up to 125 yields a significant increase in the admission rates; whilst values after 133 yields a significant decrease in admission rates. We can plot our final output for N02 and its impact on respiratory admission as well: # extract posterior estimates and values for NO2 &amp; create a data frame beta_coef_NO2 &lt;- data.frame(no2 = smooth_plot$`mu: s(NO2)`[1], beta = smooth_plot$`mu: s(NO2)`[4], lower = smooth_plot$`mu: s(NO2)`[6], lower = smooth_plot$`mu: s(NO2)`[7]) # generate a plot with coefficients and NO2 plot(beta_coef_NO2$NO2, beta_coef_NO2$estimate__, type=&quot;l&quot;, ylab = &quot;Coefficient for NO2&quot;, xlab=&quot;Observed NO2&quot;, ylim=c(-2, 3), main = &quot;GAM: Impact Respiratory burden in Turin&quot;) # add 95% credibility limits lines(beta_coef_NO2$NO2, beta_coef_NO2$upper__,col=&quot;grey&quot;) lines(beta_coef_NO2$NO2, beta_coef_NO2$lower__,col=&quot;grey&quot;) # Add null value abline(h=0, lty = &quot;dashed&quot;, col =&quot;red&quot;) Figure output Interpretation: The overall impact of N02 on respiratory-related admissions show an increasing trend across it’s values that is not signficant (4.35, 95% CrI: -5.43 to 14.39). The overall patterns can be described as U-shaped. It should be noted that the NO2 levels between 19.72 and 41.32, and past 127 yields a significant increase in the admission rates; whilst values between 55.06 to 117 yields a significant decrease in admission rates. 4.4 Printing Stan code from brms::stancode() The Stan code for the GAM model can be printed using the stancode function: # This prints the stancode stancode(model.bayes.gam) Printed Stan Code // generated with brms 2.18.0 functions { } data { int&lt;lower=1&gt; N; // total number of observations int Y[N]; // response variable // data for splines int Ks; // number of linear effects matrix[N, Ks] Xs; // design matrix for the linear effects // data for spline s(PM10) int nb_1; // number of bases int knots_1[nb_1]; // number of knots // basis function matrices matrix[N, knots_1[1]] Zs_1_1; // data for spline s(CO) int nb_2; // number of bases int knots_2[nb_2]; // number of knots // basis function matrices matrix[N, knots_2[1]] Zs_2_1; // data for spline s(NO2) int nb_3; // number of bases int knots_3[nb_3]; // number of knots // basis function matrices matrix[N, knots_3[1]] Zs_3_1; int prior_only; // should the likelihood be ignored? } transformed data { } parameters { real Intercept; // temporary intercept for centered predictors vector[Ks] bs; // spline coefficients // parameters for spline s(PM10) // standarized spline coefficients vector[knots_1[1]] zs_1_1; real&lt;lower=0&gt; sds_1_1; // standard deviations of spline coefficients // parameters for spline s(CO) // standarized spline coefficients vector[knots_2[1]] zs_2_1; real&lt;lower=0&gt; sds_2_1; // standard deviations of spline coefficients // parameters for spline s(NO2) // standarized spline coefficients vector[knots_3[1]] zs_3_1; real&lt;lower=0&gt; sds_3_1; // standard deviations of spline coefficients } transformed parameters { // actual spline coefficients vector[knots_1[1]] s_1_1; // actual spline coefficients vector[knots_2[1]] s_2_1; // actual spline coefficients vector[knots_3[1]] s_3_1; real lprior = 0; // prior contributions to the log posterior // compute actual spline coefficients s_1_1 = sds_1_1 * zs_1_1; // compute actual spline coefficients s_2_1 = sds_2_1 * zs_2_1; // compute actual spline coefficients s_3_1 = sds_3_1 * zs_3_1; lprior += student_t_lpdf(Intercept | 3, 3, 2.5); lprior += student_t_lpdf(sds_1_1 | 3, 0, 2.5) - 1 * student_t_lccdf(0 | 3, 0, 2.5); lprior += student_t_lpdf(sds_2_1 | 3, 0, 2.5) - 1 * student_t_lccdf(0 | 3, 0, 2.5); lprior += student_t_lpdf(sds_3_1 | 3, 0, 2.5) - 1 * student_t_lccdf(0 | 3, 0, 2.5); } model { // likelihood including constants if (!prior_only) { // initialize linear predictor term vector[N] mu = rep_vector(0.0, N); mu += Intercept + Xs * bs + Zs_1_1 * s_1_1 + Zs_2_1 * s_2_1 + Zs_3_1 * s_3_1; target += poisson_log_lpmf(Y | mu); } // priors including constants target += lprior; target += std_normal_lpdf(zs_1_1); target += std_normal_lpdf(zs_2_1); target += std_normal_lpdf(zs_3_1); } generated quantities { // actual population-level intercept real b_Intercept = Intercept; } "],["bayesian-hierarchical-regression-models.html", "5 Bayesian Hierarchical Regression Models 5.1 Lecture recording (Length: 48:25 minutes) 5.2 Introduction 5.3 Data preparation and set-up for Bayesian analysis in Stan 5.4 Task", " 5 Bayesian Hierarchical Regression Models 5.1 Lecture recording (Length: 48:25 minutes) Important Notes: If this or any subsequent video don’t play from this website, then try playing it directly from Microsoft Streams - you should be able to view it by clicking this [LINK]. Access to Microsoft Steams to these videos will require use of your UCL institutional login details. 5.2 Introduction This week we will learn how to perform Hierarchical regression modelling within a Bayesian framework. These models are useful and much superior to the generic regressions especially if there is an hierarchical structure to the data. This artefact must be taken into account off to ensure statistical robustness. We will focus on implementing 2-level hierarchical regressions with examples on random-intercept-only model as these are quite simple to pull off in Stan. Let us begin. 5.2.1 Datasets &amp; setting up the work directory Go to your folder GEOG0125 and create a sub folder called “Week 7” within the GEOG0125 folder. Here, we will store all our R &amp; Stan scripts as well as datasets. Set your work directory to Week 7’s folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0125/Week 7&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0125/Week 7&quot;) The dataset for this practical are: Maths attainment and activity study.csv Pregnancies and hospital data.csv The dataset were going to start of with is the Maths attainment and activity study.csv. Let us load this dataset into memory: # load up the data Maths_dataset &lt;- read.csv(&quot;`Maths attainment and activity study.csv`&quot;) 5.2.2 Loading packages We need to load the installed packages including rstan and tidybayes: # load up packages library(&#39;rstan&#39;) library(&#39;tidybayes&#39;) As usual, configure Stan: # configure Stan options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) 5.3 Data preparation and set-up for Bayesian analysis in Stan We are going to fit a 2-level hierarchical model on a continous outcome i.e., MathScore and see how the following independent variables i.e., ActiveTime and Supportive impact it. The Maths_dataset contains a total of 200 students evenly clustered into 4 classrooms ClassroomID (i.e., 50 students in each classroom). The students have a unique student number i.e., StudentID and classroom number the GroupID column. We want fit an random-intercept-only to explore the difference in the classroom’s performance in maths, as well as how ActiveTime and Supportive impact it. To perform the 2-level hierarchical model, we need to perform the dataset accordingly by extract right information from the Maths_dataset data frame for the Stan script as follows: N = total number of students (level 1); CL = maximum number of group (level 2); ClassroomID = the list of all the group numbers that align with the students; k = the total k-number of indepedent variables to be included in the model (k=2) because we want to use ActiveTime and Supportive; MathScore = we want to extract the dependent variable; ActiveTime = we want to extract the first independent variable; Supportive = we want to extract the second independent variable; We can condense this information into list() object in line of code as our dataset for Stan to compile: stan.dataset &lt;- list(N=nrow(Maths_dataset), CL=max(unique(Maths_dataset$GroupID)), ClassroomID=as.integer(Maths_dataset$GroupID), k=2, MathScore=Maths_dataset$Math, ActiveTime=Maths_dataset$ActiveTime, Supportive=Maths_dataset$Support ) At this point, the dataset is fully prepared and ready to go. Let’s create our Stan script for running a 2-level Multilevel linear regression within a Bayesian framework. 5.3.1 Creating a script to run a 2-level Multilevel linear regression in Stan We will need Data, Parameters and Model blocks only for this analysis. Recall that the Data, Parameters and Model block must be specified for any regression analysis to work. FIRST STEP: We specify the total number of observations int N as well as the number of independent variables int k as an integer in the data block. We define our MathScore outcome as a vector of size N, and we do the same for the two independent variables ActiveTime and Supportive. We need to specific the number of classrooms for which the students are clustered in through CL and ClassroomID. data { int&lt;lower = 0&gt; N; // Number of students int&lt;lower = 0&gt; CL; // Total number of classrooms int&lt;lower = 0, upper = CL&gt; ClassroomID[N]; // Aligning the classroom ID number to the student ID int&lt;lower = 0&gt; k; // Specify total number of independent variables to be used in model real&lt;lower = 0&gt; ActiveTime[N]; // First independent variable real&lt;lower = 0&gt; Supportive[N]; // Second independent variable real&lt;lower = 0&gt; MathScore[N]; // Outcome or dependent variable } SECOND STEP: For the parameters block, here we will need to specify the name of the fixed effect part of regression i.e., gamma00 and the level-1 coefficients beta for our independent variables. We need to also specify the random effect part of the model as well i.e., u0,1 u0,2, u0,3 and u0,4 as a vector by linking this to the four number of classes. Finally, we need to specify the sigma_error and group_error as the variance (or residual errors on group- and individual-level) as we must estimate this! Here is the code: data { int&lt;lower = 0&gt; N; // Number of students int&lt;lower = 0&gt; CL; // Total number of classrooms int&lt;lower = 0, upper = CL&gt; ClassroomID[N]; // Aligning the classroom ID number to the student ID int&lt;lower = 0&gt; k; // Specify total number of independent variables to be used in model real&lt;lower = 0&gt; ActiveTime[N]; // First independent variable real&lt;lower = 0&gt; Supportive[N]; // Second independent variable real&lt;lower = 0&gt; MathScore[N]; // Outcome or dependent variable } parameters { real gamma00; // Declare the fixed part (intercept) vector[k] beta; // Declare the fixed part (coefficients for ActiveTime and Supportive) vector[CL] u; // Declare the random effects u0,1 u0,2, u0,3 and u0,4 real&lt;lower = 0&gt; sigma_error; // Declare the random part (level-1) real&lt;lower = 0&gt; group_error; // Declare the random part (level-2) } THIRD AND LAST STEP: We build our likelihood function and specify the priors for each parameter under the model block. We are using a typical linear model as we are assuming there’s some linear relationship: data { int&lt;lower = 0&gt; N; // Number of students int&lt;lower = 0&gt; CL; // Total number of classrooms int&lt;lower = 0, upper = CL&gt; ClassroomID[N]; // Aligning the classroom ID number to the student ID int&lt;lower = 0&gt; k; // Specify total number of independent variables to be used in model real&lt;lower = 0&gt; ActiveTime[N]; // First independent variable real&lt;lower = 0&gt; Supportive[N]; // Second independent variable real&lt;lower = 0&gt; MathScore[N]; // Outcome or dependent variable } parameters { real gamma00; // Declare the fixed part (intercept) vector[k] beta; // Declare the fixed part (coefficients for ActiveTime and Supportive) vector[CL] u; // Declare the random effects u0,1 u0,2, u0,3 and u0,4 real&lt;lower = 0&gt; sigma_error; // Declare the random part (level-1) real&lt;lower = 0&gt; group_error; // Declare the random part (level-2) } model { real mu; u ~ normal(0, group_error); gamma00 ~ normal(0, 20); beta ~ normal(0, 20); for (i in 1:N) { mu = gamma00 + u[ClassroomID[i]] + beta[1]*ActiveTime[i] + beta[2]*Supportive[i]; MathScore[i] ~ normal(mu, sigma_error); } } Let’s save the script as Maths_Activity_study.stan. Now, we can compile and run it through RStudio to get our estimates coefficients and random intercept results. 5.3.2 Compiling our Stan code for Hierarchical Modelling Now, let us turn our attention to RStudio. Using the stan() to compile the save script to obtain the posterior estimation of the parameters in our hierarchical model: # Start the clock ptm &lt;- proc.time() # compile linear regression model for now bayesian.hierarchical.model = stan(&quot;Maths_Activity_study.stan&quot;, data=stan.dataset, iter=10000, chains=6, verbose = FALSE) # Stop the clock proc.time() - ptm Output summary table bayesian.hierarchical.model Inference for Stan model: anon_model. 6 chains, each with iter=10000; warmup=5000; thin=1; post-warmup draws per chain=5000, total post-warmup draws=30000. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat gamma00 33.24 0.38 24.31 -18.18 16.03 35.80 54.05 68.28 4194 1 beta[1] 11.44 0.01 0.78 9.90 10.92 11.45 11.97 12.98 18542 1 beta[2] 3.18 0.01 0.83 1.57 2.63 3.18 3.75 4.82 18370 1 u[1] 44.92 0.38 24.33 9.76 24.10 42.30 62.08 96.37 4182 1 u[2] 37.23 0.38 24.33 2.07 16.42 34.62 54.48 88.78 4192 1 u[3] 21.59 0.38 24.32 -13.54 0.79 18.96 38.76 73.01 4195 1 u[4] 28.08 0.38 24.32 -7.09 7.28 25.49 45.30 79.49 4190 1 sigma_error 3.34 0.00 0.17 3.03 3.22 3.33 3.45 3.69 17383 1 group_error 56.16 0.73 58.88 8.34 20.69 40.83 71.25 197.13 6566 1 lp__ -353.89 0.02 2.14 -359.01 -355.06 -353.55 -352.34 -350.73 9263 1 Samples were drawn using NUTS(diag_e) at Fri Mar 3 06:03:06 2023. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Here is the interpretation: gamma00 is the global mean (or average) in the population under study. This means after for accounting for clustering, on average the students’ marks in maths are 33.24 (95% CrI: -18.18 to 68.28). Note that the u[1] is the random-effect for the intercept for Class 1, its value is 44.92 (i.e., positive value). By added this to 33.24, we get the intercept that is specific to Class 1. Similarly, for u[2] by adding 37.23 to 33.24, we obtain the intercept this is specific to Class 2, and the rest is so on, and so forth. beta[1] is the level-1 coefficient for ActiveTime. This means after for accounting for clustering, its yields a positive increase on average for maths scores the more time is spent on active learning. This increase is significant since the credibility intervals exclude the null value of zero. beta[2], in the same vein, the same can be said for amount of one-to-one support a student receives from a teacher. Variances for level-1 and level-2 can used to explain the model’s performance by taking the 56.16/(56.16 + 3.34) = 0.943. Here, it’s 0.943 or 94.3%, this quantity is actually known as the Intraclass Correlation Coefficient (ICC). This the proportion of explained variation in the 2-level hierarchical model when accounting for group clustering. Here, the value is quite high and so its a good model. Anything close to 1 (100%) is good and bad vice versa. 5.4 Task Try your hand on this problem in Stan: Build a random intercept logit model using data on 1060 births to 501 mothers. The outcome of interest is whether the birth was delivered in a hospital or elsewhere. The predictors include the log of income loginc, the distance to the nearest hospital distance, and two indicators of mothers’s education: dropout for less than high school and college for college graduate, so high school or some college is the reference cell. Have ago at building this model. "],["bayesian-spatial-modelling-for-areal-data-in-stan.html", "6 Bayesian Spatial Modelling for Areal Data in Stan 6.1 Lecture recording (Length: 40:58 minutes) 6.2 Introduction 6.3 Learning outcomes 6.4 Demonstration recording (Length: 51:46 minutes) 6.5 Before we start… 6.6 Data preparation in RStudio 6.7 Creating the script for the Spatial ICAR smoothed model 6.8 Compiling Stan code for Spatial ICAR modelling 6.9 Task", " 6 Bayesian Spatial Modelling for Areal Data in Stan 6.1 Lecture recording (Length: 40:58 minutes) Important Notes: If this or any subsequent video don’t play from this website, then try playing it directly from Microsoft Streams - you should be able to view it by clicking this [LINK]. Access to Microsoft Steams to these videos will require use of your UCL institutional login details. 6.2 Introduction This week’s practical you will be introduced to applying spatial Bayesian models for risk assessments for areal-level discrete outcomes. This a powerful tool used often in many applications e.g., spatial epidemiology, disaster risk reduction and environmental criminology and many more. This exercise will focus on casualty data resulting from road accidents by car users (and/or occupants) in the UK. Road traffic accidents &amp; injuries are a serious problem worldwide. In fact, it is one of the leading causes of death attributed to significant life-years lost and as a consequence is associated with considerable economic losses to impacted individuals and families. There are many individual, environmental and geographical risk factors for road-related casualties. Here, we will use a series of spatial models from a Bayesian framework to estimate the area-specific relative risks (RR) of casualties due to road accidents in local authority areas across the England; and we will quantify the levels of uncertainty using a device called exceedance probabilities. 6.3 Learning outcomes You will learn how to: Implement the Spatial intrinsic conditional autoregressive model (ICAR) to areal data in RStan; How to use the ICAR model to predict the area-specific relative risks (RR) for areal units and how to determine whether the levels of such risk are statistically significant or not through 95% credible intervals (95% CrI); How to determine the Exceedance Probability i.e., the probability that an area has an excess risk of an outcome that exceeds a given risk threshold (e.g., RR &gt; 1 (null value)); You can follow the live walkthrough demonstration delivered in the first 1-hour of the practical, and then use the remaining half to try the practical tutorials and follow the instructions on your own. 6.4 Demonstration recording (Length: 51:46 minutes) Important Notes: If this or any subsequent video don’t play from this website, then try playing it directly from Microsoft Streams - you should be able to view it by clicking this [LINK]. Access to Microsoft Steams to these videos will require use of your UCL institutional login details. 6.5 Before we start… 6.5.1 Loading and installing packages We will need to install the following new packages: SpatialEpi: grants access to function expected() to calculated expected numbers tidybayes: grants access to further functions for managing posterior estimates. We will need it calculating the exceedance probabilities. Load this alongside tidyverse package geostan: grants access to further functions that we need to compute the adjacency matrix that can be handled in Stan. We will use the two functions shape2mat() and prep_icar_data() to create the adjacency matrix as nodes and edges. install.packages(&quot;geostan&quot;) install.packages(&quot;SpatialEpi&quot;) install.packages(&quot;tidybayes&quot;) Now, lets load all packages need specifically for this computer practical: # Load the packages with library() library(&quot;sf&quot;) library(&quot;tmap&quot;) library(&quot;spdep&quot;) library(&quot;rstan&quot;) library(&quot;geostan&quot;) library(&quot;SpatialEpi&quot;) library(&quot;tidybayes&quot;) library(&quot;tidyverse&quot;) Upon loading the rstan package, we highly recommend using this code to configure it with RStudio: options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) This tells RStudio to use the multiple cores in your computer for parallel processing whenever Stan is being implemented. Every time you want to use Stan - make sure to load parallel::detectCores() and rstan_options code. Important Notes: Make sure to be using the latest version of RStan (2.26.13). Check in your list of packages. If you are using the old version (i.e., 2.21.1) you may encounter problems with creating the functions in Stan script. It is recommended that you remove version 2.21.1 and install the latest development version 2.26.13. See why [LINK] Instruction to do this are as follows: # remove rstan and StanHeaders remove.packages(c(&quot;StanHeaders&quot;, &quot;rstan&quot;)) # install the latest development version 2.26.13 from direct source install.packages(&quot;rstan&quot;, repos = c(&quot;https://mc-stan.org/r-packages/&quot;, getOption(&quot;repos&quot;))) 6.5.2 Datasets &amp; setting up the work directory Go to your folder GEOG0125 and create a sub folder called “Week 8” within the GEOG0125 folder. Here, we will store all our R &amp; Stan scripts as well as datasets. Set your work directory to Week 8’s folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0125/Week 8&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0125/Week 8&quot;) The dataset for this practical are: Road Casualties Data 2015-2020.csv England Local Authority Shapefile.shp England Regions Shapefile.shp The dataset were going to start of with is the Road Casualties Data 2015-2020.csv. This data file contains the following information: It contains the 307 local authority areas that operate in England. The names and codes are defined under the columns LAD21NM and LAD21CD respectively; It contains the following variables: Population, Casualties and IMDScore. The Casualties is the dependent variable, and IMDScore is the independent variable. We will need Population column to derive the expected number of road casualties to be used as an offset in the Bayesian model. The shapefile England Local Authority Shapefile.shp contains the boundaries for all 307 local authorities in England. The England Regions Shapefile.shp contains the boundaries for all 10 regions that make up England. Let us load these dataset to memory: # load the shape files england_LA_shp &lt;- read_sf(&quot;England Local Authority Shapefile.shp&quot;) england_Region_shp &lt;- read_sf(&quot;England Regions Shapefile.shp&quot;) # load in the cross-sectional road accident dataset road_accidents &lt;- read.csv(&quot;Road Casualties Data 2015-2020.csv&quot;) 6.6 Data preparation in RStudio 6.6.1 Calculation for expected numbers In order to estimate the risk of casualties due to road accidents at an LA-level in England, we will need to first obtain a column that contains estimates from expected number of road casualties. This is derived from the Population column which as denominators or reference population size which is multiplied to the overall incidence rates of road accidents to get the number of expected casualties for each LA area. You can use the expected() function to compute this column into the road_accident data frame # calculate the expected number of cases road_accidents$ExpectedNum &lt;- round(expected(population = road_accidents$Population, cases = road_accidents$Casualties, n.strata = 1), 0) This particular column ExpectedNum is important, it must be computed and used as an offset in our spatial model. 6.6.2 Converting the spatial adjacency matrix to nodes &amp; edges We will need to transform the image below into a list of nodes and edges accordingly as Stan can only identify adjacency with a set of paired nodes with edges that connect them. For instance, node1 is the index region and node2 is the list of neighbouring regions connected to the index region in node1 We can perform this by first merging in the road accident data to the LA-level shapefile. Once this action is completed, we will then need to coerce the spatial object to be from a simple features (i.e., sf) object to the spatial object (i.e., sp). Here is the code: # merge the attribute table to the shapefile spatial.data &lt;- merge(england_LA_shp, road_accidents, by.x = c(&quot;LAD21CD&quot;, &quot;LAD21NM&quot;), by.y = c(&quot;LAD21CD&quot;, &quot;LAD21NM&quot;), all.x = TRUE) # reordering the columns spatial.data &lt;- spatial.data[, c(3,1,2,4,5,7,6)] # need to be coerced into a spatial object sp.object &lt;- as(spatial.data, &quot;Spatial&quot;) Now, we are going to need the nodes and edges from the sp.object using the shape2mat() function - this changes it into a matrix object. From the matrix object, we will be able to prepare the data from spatial ICAR model using the prep_icar_data() function. Here, is the code: # needs to be coerced into a matrix object adjacencyMatrix &lt;- shape2mat(sp.object) # we extract the components for the ICAR model extractComponents &lt;- prep_icar_data(adjacencyMatrix) From the extractComponents object, we will need to extract the following contents: $group_size this is the number of areal units under observation listed in the shapefile (should be the same in the road accidents dataset) $node1 are index regions of interest $node2 are the other neighbouring regions that are connected to the index region of interest listed in node1 $n_edges creates the network as show area is connected to what neighbourhood. It’s still an adjacency matrix using the queen contiguity matrix but as a network. Here is the code for performing the extraction: n &lt;- as.numeric(extractComponents$group_size) nod1 &lt;- extractComponents$node1 nod2 &lt;- extractComponents$node2 n_edges &lt;- as.numeric(extractComponents$n_edges) Note that the information needed are stored in n, nod1, nod2 and n_edges. 6.6.3 Create the dataset to be compiled in Stan For the list step in the data preparation, we need to define the variables needed to be compiled in Stan. The outcome Casualties, independent variable IMDScore and offset variable ExpectedNum needs to be extracted into separate vectors. The data needs to be aligned with the areas in shapefile as the result will be churned to that order. So make sure the data is already linked in to the geometries! Here is the code: y &lt;- spatial.data$Casualties x &lt;- spatial.data$IMDScore e &lt;- spatial.data$ExpectedNum Now, we create our dataset for Stan: # put all components into a list object stan.spatial.dataset &lt;- list(N=n, N_edges=n_edges, node1=nod1, node2=nod2, Y=y, X=x, E=e) The above information is going to be passed to Stan in the data block. Now, we are in the position to develop our spatial intrinsic conditional autoregressive (ICAR) model. Now open your Stan script and we begin. 6.7 Creating the script for the Spatial ICAR smoothed model 6.7.1 Defining the ICAR component of the Spatial model Here, we need to create a function for the ICAR component in our model to define the prior of spatial random effect \\(\\phi\\). This is essentially a probability density computed from the sum of the squared pairwise differences between the spatial effect in area \\(\\phi_i\\) against another \\(\\phi_j\\). This bit is also a reconstruction for the adjacency matrix. It is a function based on nodes1, nodes2 (i.e., list of adjacent region/area pairs) and the exact number of areas N in a study area. We create the function as icar_normal_lpdf(), where the inputs will be N, nodes1 and nodes2to get phi. In the model block (as you will soon see) - we will specify the prior for \\(\\phi\\) as phi ~ icar_normal(N, node1, node2). It should be noted that in this function phi is a vector, N is a simple integer, and the node1 and node2 is an array. The following program fragment shows the Stan parameter and model block to compute the spatial effects vector through the statement phi ~ icar_normal(N, node1, node2) (which you will see later): functions { real icar_normal_lpdf(vector phi, int N, array[] int node1, array[] int node2) { return -0.5 * dot_self(phi[node1] - phi[node2]); } } Important Notes: Use the above function as this will help you to generate the spatial random effects for each region 6.7.2 Adding the blocks in our ICAR Stan script 6.7.2.1 Data block In the data block, we specify the following: The total number of areal unit observations N as an integer (i.e., 307); The total number edges N_edges as an integer (i.e., 823); The nodes1 based on the size of N_edges (i.e., 823) must be specified as an array to connect with nodes2; The nodes2 based on the size of N_edges (i.e., 823) must be specified as an array to connect with nodes1; We define our Y outcome (i.e., road accidents) as an array of size N (i.e., 307) which is an integer; We define our independent variables X as a vector of size N (i.e., 307); We must define our offset for the expected numbers E as a vector of size N (i.e., 307); Here is what our model block will look like: functions { real icar_normal_lpdf(vector phi, int N, array[] int node1, array[] int node2) { return -0.5 * dot_self(phi[node1] - phi[node2]); } } data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; N_edges; array[N_edges] int&lt;lower=1, upper=N&gt; node1; array[N_edges] int&lt;lower=1, upper=N&gt; node2; array[N] int&lt;lower=0&gt; Y; // dependent variable i.e., number of road accidents vector&lt;lower=0&gt;[N] X; // independent variable i.e., deprivation score vector&lt;lower=0&gt;[N] E; // estimated number of expected cases of road accidents } 6.7.2.2 Transformed data block We are going a transformed data block to our script. Here, we are simply changing the expected numbers by taking its log() and creating another vector log_offset. This will be added to the poisson_log() sampling statement in our likelihood function of the spatial model. Here, we specify it as follows: functions { real icar_normal_lpdf(vector phi, int N, array[] int node1, array[] int node2) { return -0.5 * dot_self(phi[node1] - phi[node2]); } } data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; N_edges; array[N_edges] int&lt;lower=1, upper=N&gt; node1; array[N_edges] int&lt;lower=1, upper=N&gt; node2; array[N] int&lt;lower=0&gt; Y; // dependent variable i.e., number of road accidents vector&lt;lower=0&gt;[N] X; // independent variable i.e., deprivation score vector&lt;lower=0&gt;[N] E; // estimated number of expected cases of road accidents } transformed data { vector[N] log_offset = log(E); // use the expected cases as an offset and add to the regression model } 6.7.2.3 Parameters block For the parameters block, we will need to specify the following: The global intercept i.e., alpha for the entire study area (i.e., average risk of road accidents on a population-level); The coefficient beta for our independent variable X which is the IMDScore; We also specify sigma as a real value which is some error or standard deviation that is multiplied to the spatial effects phi; We define the spatial effects phi to be vector of size N; We add the block as follows: functions { real icar_normal_lpdf(vector phi, int N, array[] int node1, array[] int node2) { return -0.5 * dot_self(phi[node1] - phi[node2]); } } data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; N_edges; array[N_edges] int&lt;lower=1, upper=N&gt; node1; array[N_edges] int&lt;lower=1, upper=N&gt; node2; array[N] int&lt;lower=0&gt; Y; // dependent variable i.e., number of road accidents vector&lt;lower=0&gt;[N] X; // independent variable i.e., deprivation score vector&lt;lower=0&gt;[N] E; // estimated number of expected cases of road accidents } transformed data { vector[N] log_offset = log(E); // use the expected cases as an offset and add to the regression model } parameters { real alpha; // define the intercept (overall risk in population) real beta; // define the coefficient for the deprivation score variable real&lt;lower=0&gt; sigma; // define the overall standard deviation producted with spatial effect smoothing term phi vector[N] phi; // spatial effect smoothing term or spatial ICAR component of the model } 6.7.2.4 Model block We build our likelihood function and specify the priors for each parameter under the model block. We are using a typical Poisson model with a log link function as we are assuming there’s some linear relationship between the counts and IMD score, but here we are also taking into account the spatial structure in the vector parameter for phi: functions { real icar_normal_lpdf(vector phi, int N, array[] int node1, array[] int node2) { return -0.5 * dot_self(phi[node1] - phi[node2]); } } data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; N_edges; array[N_edges] int&lt;lower=1, upper=N&gt; node1; array[N_edges] int&lt;lower=1, upper=N&gt; node2; array[N] int&lt;lower=0&gt; Y; // dependent variable i.e., number of road accidents vector&lt;lower=0&gt;[N] X; // independent variable i.e., deprivation score vector&lt;lower=0&gt;[N] E; // estimated number of expected cases of road accidents } transformed data { vector[N] log_offset = log(E); // use the expected cases as an offset and add to the regression model } parameters { real alpha; // define the intercept (overall risk in population) real beta; // define the coefficient for the deprivation score variable real&lt;lower=0&gt; sigma; // define the overall standard deviation producted with spatial effect smoothing term phi vector[N] phi; // spatial effect smoothing term or spatial ICAR component of the model } model { phi ~ icar_normal(N, node1, node2); // prior for the spatial random effects Y ~ poisson_log(log_offset + alpha + beta*X + phi*sigma); // likelihood function i.e., spatial ICAR model using Possion distribution alpha ~ normal(0.0, 1.0); // prior for intercept (weak/uninformative prior) beta ~ normal(0.0, 1.0); // prior for coefficient (weak/uninformative prior) sigma ~ normal(0.0, 1.0); // prior for SD (weak/uninformative prior) sum(phi) ~ normal(0, 0.001*N); } 6.7.2.5 Generated quantities block Lastly, we instruct Stan on the parameters we want to report. We want them as relative risk ratio (RR). We use the generated quantities block to obtain these estimates by exponentiation of our regression model: functions { real icar_normal_lpdf(vector phi, int N, array[] int node1, array[] int node2) { return -0.5 * dot_self(phi[node1] - phi[node2]); } } data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; N_edges; array[N_edges] int&lt;lower=1, upper=N&gt; node1; array[N_edges] int&lt;lower=1, upper=N&gt; node2; array[N] int&lt;lower=0&gt; Y; // dependent variable i.e., number of road accidents vector&lt;lower=0&gt;[N] X; // independent variable i.e., deprivation score vector&lt;lower=0&gt;[N] E; // estimated number of expected cases of road accidents } transformed data { vector[N] log_offset = log(E); // use the expected cases as an offset and add to the regression model } parameters { real alpha; // define the intercept (overall risk in population) real beta; // define the coefficient for the deprivation score variable real&lt;lower=0&gt; sigma; // define the overall standard deviation producted with spatial effect smoothing term phi vector[N] phi; // spatial effect smoothing term or spatial ICAR component of the model } model { phi ~ icar_normal(N, node1, node2); // prior for the spatial random effects Y ~ poisson_log(log_offset + alpha + beta*X + phi*sigma); // likelihood function i.e., spatial ICAR model using Possion distribution alpha ~ normal(0.0, 1.0); // prior for intercept (weak/uninformative prior) beta ~ normal(0.0, 1.0); // prior for coefficient (weak/uninformative prior) sigma ~ normal(0.0, 1.0); // prior for SD (weak/uninformative prior) sum(phi) ~ normal(0, 0.001*N); } generated quantities { vector[N] eta = alpha + beta*X + phi*sigma; // do eta equals alpha + beta*X + phi*sigma to get the relative risk for areas vector[N] mu = exp(eta); // the exponentiate eta to mu areas-specific relative risk ratios (RRs) } Well done! You have coded your first spatial risk model. Alright, let us save the script as icar_poisson_model.stan. We can now compile and run it through RStudio to get our posterior estimates as risk ratios (RR) for each areas. We can also get the exceedance probabilities. The next steps are easy from this point onwards. 6.8 Compiling Stan code for Spatial ICAR modelling 6.8.1 Printing of the global results Now, let us turn our attention to RStudio. Using the stan() to compile the saved script to obtain the posterior estimation of the parameters from our model: icar_poisson_fit = stan(&quot;icar_poisson_model.stan&quot;, data=stan.spatial.dataset, iter=20000, chains=6, verbose = FALSE) We can see our estimated results for alpha, beta and sigma: # remove that annoying scientific notation options(scipen = 999) summary(icar_poisson_fit, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;), probs=c(0.025, 0.975))$summary Output from summary()$summary function: mean se_mean sd 2.5% 97.5% n_eff Rhat alpha 0.132105883 0.0046872419 0.085604021 -0.04109926 0.3001698223 333.5444 1.020028 beta -0.008203683 0.0002366832 0.004319974 -0.01669401 0.0005396596 333.1404 1.020069 sigma 1.032680985 0.0017143449 0.047901705 0.94219323 1.1295132135 780.7384 1.004863 Here is the interpretation: alpha is the global mean (or average) in the population under study. It means on average the road accident occurrence in England for the period 2015 to 2020 is 0.1321 (95% CrI: -0.0411 to 0.3002). If we take the exponent of this value i.e., exp(0.132105883) - we get the relative risks of road accidents which is 1.14 times higher in England (95% CrI: 0.959 to 1.35). The result is not significant as the null value exists between these limits. beta is the coefficient for IMDScore. This means that its yields a decrease on average for road accidents throughout England for more deprived areas -0.0082 (95% CI: -0.016 to 0.00053). This is negligible and not significant; and if you exponentiate this value its really close to the null value (1). sigma is the overall standard deviation or global error. Note, we can view the spatial effects i.e., phi for each area using this code: # show first 6 rows only instead of the full 307 head(summary(icar_poisson_fit, pars=c(&quot;phi&quot;), probs=c(0.025, 0.975))$summary) Alternatively, you can use the print() function to get a detailed output: print(icar_poisson_fit, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;phi&quot;), probs=c(0.025, 0.975)) Output from print() function: Inference for Stan model: anon_model. 6 chains, each with iter=20000; warmup=10000; thin=1; post-warmup draws per chain=10000, total post-warmup draws=60000. mean se_mean sd 2.5% 97.5% n_eff Rhat alpha 0.13 0 0.09 -0.04 0.30 334 1.02 beta -0.01 0 0.00 -0.02 0.00 333 1.02 sigma 1.03 0 0.05 0.94 1.13 781 1.00 Samples were drawn using NUTS(diag_e) at Wed Mar 8 21:06:27 2023. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). 6.8.2 Rapid diagnostics of the rHATs Before mapping the relative risks, we must check if the any of the estimates i.e., alpha, beta, sigma and all phi exceed the rHAT value of 1.1. This is an indication that iterations did not perform well if an rHAT for a parameter is above 1.1. We can do a rapid checks to see which parameter is valid or not by creating a binary variable of 1’s (Valid) and 0’s (Not valid). We can tabulate it to see the numbers: # diagnostic check on the rHats - put everything into a data frame diagnostic.checks &lt;- as.data.frame(summary(icar_poisson_fit, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;phi&quot;, &quot;lp__&quot;), probs=c(0.025, 0.5, 0.975))$summary) # create binary variable diagnostic.checks$valid &lt;- ifelse(diagnostic.checks$Rhat &lt; 1.1, 1, 0) # tabulate it table(diagnostic.checks$valid) Everything is okay - all outputted parameters have an rHAT &lt; 1.1. We are free to generate maps. NOTES: To avoid such complications, it is always to best to run about 10000, 15000 or more iterations. Usually, shorter iterations yield low effective sample sizes after thinning/warm-up samples are discarded, which in turn, may lead to complications that may cause the rHAT to be above 1.1. 6.8.3 Extraction of the area-specific relative risks If you run the following code: # show first 6 rows only instead of the full 307 head(summary(icar_poisson_fit, pars=c(&quot;mu&quot;), probs=c(0.025, 0.975))$summary) We see the relative risk (RR) estimates for the first areas under the column mu with their corresponding credibility limits under the 2.5% and 97.5% column. We are going to extract this information into a data frame and applying the cleaning and renaming of columns accordingly: # extraction key posterior results for the generated quantities relativeRisk.results &lt;- as.data.frame(summary(icar_poisson_fit, pars=c(&quot;mu&quot;), probs=c(0.025, 0.975))$summary) # now cleaning up this table up # first, insert clean row numbers to new data frame row.names(relativeRisk.results) &lt;- 1:nrow(relativeRisk.results) # second, rearrange the columns into order relativeRisk.results &lt;- relativeRisk.results[, c(1,4,5,7)] # third, rename the columns appropriately colnames(relativeRisk.results)[1] &lt;- &quot;rr&quot; colnames(relativeRisk.results)[2] &lt;- &quot;rrlower&quot; colnames(relativeRisk.results)[3] &lt;- &quot;rrupper&quot; colnames(relativeRisk.results)[4] &lt;- &quot;rHAT&quot; # view clean table head(relativeRisk.results) See clean table: rr rrlower rrupper rHAT valid 1 0.6295692 0.5759348 0.6859993 0.9999354 1 2 0.8043656 0.7550300 0.8555160 0.9999809 1 3 0.6518326 0.6068501 0.6991145 0.9999186 1 4 0.6292392 0.5920648 0.6674334 0.9999475 1 5 0.8885551 0.8284516 0.9505547 1.0000141 1 6 0.7971718 0.7451160 0.8513495 0.9999757 1 Insert these columns into the spatial.data object as follow: # now, we proceed to generate our risk maps # align the results to the areas in shapefile spatial.data$rr &lt;- relativeRisk.results[, &quot;rr&quot;] spatial.data$rrlower &lt;- relativeRisk.results[, &quot;rrlower&quot;] spatial.data$rrupper &lt;- relativeRisk.results[, &quot;rrupper&quot;] These relative will allow us to see the mapped risks of road accidents across local authorities in England. We also want a supporting map indicate whether the risks are significant or not. Here, we create an extra column in the spatial.data called Significance. # create categories to define if an area has significant increase or decrease in risk, or nothing all spatial.data$Significance &lt;- NA spatial.data$Significance[spatial.data$rrlower&lt;1 &amp; spatial.data$rrupper&gt;1] &lt;- 0 # NOT SIGNIFICANT spatial.data$Significance[spatial.data$rrlower==1 | spatial.data$rrupper==1] &lt;- 0 # NOT SIGNIFICANT spatial.data$Significance[spatial.data$rrlower&gt;1 &amp; spatial.data$rrupper&gt;1] &lt;- 1 # SIGNIFICANT INCREASE spatial.data$Significance[spatial.data$rrlower&lt;1 &amp; spatial.data$rrupper&lt;1] &lt;- -1 # SIGNIFICANT DECREASE 6.8.4 Mapping of RR and significance The next set of codes are all cosmetics for the creating our risk map for road accidents. Here is the code: # For map design for the relative risk -- you want to understand or get a handle on what the distribution for risks look like # this would inform you of how to create the labelling for the legends when make a map in tmap summary(spatial.data$rr) hist(spatial.data$rr) # creating the labels RiskCategorylist &lt;- c(&quot;&gt;0.0 to 0.25&quot;, &quot;0.26 to 0.50&quot;, &quot;0.51 to 0.75&quot;, &quot;0.76 to 0.99&quot;, &quot;1.00 &amp; &lt;1.01&quot;, &quot;1.01 to 1.10&quot;, &quot;1.11 to 1.25&quot;, &quot;1.26 to 1.50&quot;, &quot;1.51 to 1.75&quot;, &quot;1.76 to 2.00&quot;, &quot;2.01 to 3.00&quot;) # next, we are creating the discrete colour changes for my legends and want to use a divergent colour scheme # scheme ranges from extreme dark blues to light blues to white to light reds to extreme dark reds # you can pick your own colour choices by checking out this link [https://colorbrewer2.org] RRPalette &lt;- c(&quot;#65bafe&quot;,&quot;#98cffe&quot;,&quot;#cbe6fe&quot;,&quot;#dfeffe&quot;,&quot;white&quot;,&quot;#fed5d5&quot;,&quot;#fcbba1&quot;,&quot;#fc9272&quot;,&quot;#fb6a4a&quot;,&quot;#de2d26&quot;,&quot;#a50f15&quot;) # categorising the risk values to match the labelling in RiskCategorylist object spatial.data$RelativeRiskCat &lt;- NA spatial.data$RelativeRiskCat[spatial.data$rr&gt;= 0 &amp; spatial.data$rr &lt;= 0.25] &lt;- -4 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 0.25 &amp; spatial.data$rr &lt;= 0.50] &lt;- -3 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 0.50 &amp; spatial.data$rr &lt;= 0.75] &lt;- -2 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 0.75 &amp; spatial.data$rr &lt; 1] &lt;- -1 spatial.data$RelativeRiskCat[spatial.data$rr&gt;= 1.00 &amp; spatial.data$rr &lt; 1.01] &lt;- 0 spatial.data$RelativeRiskCat[spatial.data$rr&gt;= 1.01 &amp; spatial.data$rr &lt;= 1.10] &lt;- 1 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 1.10 &amp; spatial.data$rr &lt;= 1.25] &lt;- 2 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 1.25 &amp; spatial.data$rr &lt;= 1.50] &lt;- 3 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 1.50 &amp; spatial.data$rr &lt;= 1.75] &lt;- 4 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 1.75 &amp; spatial.data$rr &lt;= 2.00] &lt;- 5 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 2.00 &amp; spatial.data$rr &lt;= 10] &lt;- 6 # check to see if legend scheme is balanced - if a number is missing that categorisation is wrong! table(spatial.data$RelativeRiskCat) Generating the maps as a paneled output: # map of relative risk rr_map &lt;- tm_shape(spatial.data) + tm_fill(&quot;RelativeRiskCat&quot;, style = &quot;cat&quot;, title = &quot;Relavtive Risk&quot;, palette = RRPalette, labels = RiskCategorylist) + tm_shape(england_Region_shp) + tm_polygons(alpha = 0.05) + tm_text(&quot;name&quot;, size = &quot;AREA&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE, legend.title.size = 0.8, legend.text.size = 0.7) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_scale_bar(position = c(&quot;right&quot;, &quot;bottom&quot;)) # map of significance regions sg_map &lt;- tm_shape(spatial.data) + tm_fill(&quot;Significance&quot;, style = &quot;cat&quot;, title = &quot;Significance Categories&quot;, palette = c(&quot;#33a6fe&quot;, &quot;white&quot;, &quot;#fe0000&quot;), labels = c(&quot;Significantly low&quot;, &quot;Not Significant&quot;, &quot;Significantly high&quot;)) + tm_shape(england_Region_shp) + tm_polygons(alpha = 0.10) + tm_text(&quot;name&quot;, size = &quot;AREA&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE, legend.title.size = 0.8, legend.text.size = 0.7) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_scale_bar(position = c(&quot;right&quot;, &quot;bottom&quot;)) # create side-by-side plot tmap_arrange(rr_map, sg_map, ncol = 2, nrow = 1) Output: 6.8.5 Extracting and mapping of the exceedance probabilities Exceedance probabilities allows the user to quantify the levels of uncertainty surrounding the risks we quantified. We can use a threshold for instance an RR &gt; 1 and ask what is the probability that an area has an excess risk of road accidents and visualise this as well. Just like the RRs, we are going to extract this information into a vector and include it into our spatial.data object. For this extraction, we will need to use functions from the tidybayes and tidyverse packages i.e., spread_draws(), group_by(), summarise() and pull(): # extract the exceedence probabilities from the icar_possion_fit object # compute the probability that an area has a relative risk ratio &gt; 1.0 threshold &lt;- function(x){mean(x &gt; 1.00)} excProbrr &lt;- icar_poisson_fit %&gt;% spread_draws(mu[i]) %&gt;% group_by(i) %&gt;% summarise(mu=threshold(mu)) %&gt;% pull(mu) # insert the exceedance values into the spatial data frame spatial.data$excProb &lt;- excProbrr The next set of codes are all cosmetics for the creating our probability exceedance map for road accidents. Here is the code: # create the labels for the probabilities ProbCategorylist &lt;- c(&quot;&lt;0.01&quot;, &quot;0.01-0.09&quot;, &quot;0.10-0.19&quot;, &quot;0.20-0.29&quot;, &quot;0.30-0.39&quot;, &quot;0.40-0.49&quot;,&quot;0.50-0.59&quot;, &quot;0.60-0.69&quot;, &quot;0.70-0.79&quot;, &quot;0.80-0.89&quot;, &quot;0.90-0.99&quot;, &quot;1.00&quot;) # categorising the probabilities in bands of 10s spatial.data$ProbCat &lt;- NA spatial.data$ProbCat[spatial.data$excProb&gt;=0 &amp; spatial.data$excProb&lt; 0.01] &lt;- 1 spatial.data$ProbCat[spatial.data$excProb&gt;=0.01 &amp; spatial.data$excProb&lt; 0.10] &lt;- 2 spatial.data$ProbCat[spatial.data$excProb&gt;=0.10 &amp; spatial.data$excProb&lt; 0.20] &lt;- 3 spatial.data$ProbCat[spatial.data$excProb&gt;=0.20 &amp; spatial.data$excProb&lt; 0.30] &lt;- 4 spatial.data$ProbCat[spatial.data$excProb&gt;=0.30 &amp; spatial.data$excProb&lt; 0.40] &lt;- 5 spatial.data$ProbCat[spatial.data$excProb&gt;=0.40 &amp; spatial.data$excProb&lt; 0.50] &lt;- 6 spatial.data$ProbCat[spatial.data$excProb&gt;=0.50 &amp; spatial.data$excProb&lt; 0.60] &lt;- 7 spatial.data$ProbCat[spatial.data$excProb&gt;=0.60 &amp; spatial.data$excProb&lt; 0.70] &lt;- 8 spatial.data$ProbCat[spatial.data$excProb&gt;=0.70 &amp; spatial.data$excProb&lt; 0.80] &lt;- 9 spatial.data$ProbCat[spatial.data$excProb&gt;=0.80 &amp; spatial.data$excProb&lt; 0.90] &lt;- 10 spatial.data$ProbCat[spatial.data$excProb&gt;=0.90 &amp; spatial.data$excProb&lt; 1.00] &lt;- 11 spatial.data$ProbCat[spatial.data$excProb == 1.00] &lt;- 12 # check to see if legend scheme is balanced table(spatial.data$ProbCat) Generating the probability map output: # map of exceedance probabilities tm_shape(spatial.data) + tm_fill(&quot;ProbCat&quot;, style = &quot;cat&quot;, title = &quot;Probability&quot;, palette = &quot;GnBu&quot;, labels = ProbCategorylist) + tm_shape(england_Region_shp) + tm_polygons(alpha = 0.05, border.col = &quot;black&quot;) + tm_text(&quot;name&quot;, size = &quot;AREA&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE, legend.title.size = 0.8, legend.text.size = 0.7) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_scale_bar(position = c(&quot;right&quot;, &quot;bottom&quot;)) Output: Example Interpretation: We can see that the risk patterns for road accidents across England is quite heterogeneous. While it is quite pronounced in all 10 regions in England, the burden is quite significant in South West region with large numbers of local authorities having an increased risk which are statistically significant. While, there’s significant limitation the models used here - perhaps, the Department for Transport should do an investigation on these patterns starting with the South West area. 6.9 Task Try your hand on this problem in Stan: Build a spatial ICAR model using data on counts of low birth weights in Georgia US to create the following maps: Map showing the relative risk of low birth weight across the 163 counties in Georgia Map showing the statistical significance of the relative risk Map showing the Exceedance Probabilities using the threshold of RR &gt; 1 Use the following dataset: Low_birth_weights_data.csv: Contains NAME, Lowbirths (Counts) and ExpectedNumber Georgia_Shapefile.shp: Contains NAME Try to complete this without consulting the solutions. "],["research-methods-study-design-and-revision.html", "7 Research Methods, Study Design and Revision 7.1 Lecture (Length: 51:03 minutes) 7.2 Revision (Length: 17:56 minutes)", " 7 Research Methods, Study Design and Revision 7.1 Lecture (Length: 51:03 minutes) [LINK] 7.2 Revision (Length: 17:56 minutes) [LINK] Important Notes: If this or any subsequent video don’t play from this website, then try playing it directly from Microsoft Streams - you should be able to view it by clicking in the bit that says [LINK] beneath the videos. Access to Microsoft Steams to these videos will require use of your UCL institutional login details. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
